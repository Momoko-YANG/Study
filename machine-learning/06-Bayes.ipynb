{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "658a18da",
   "metadata": {},
   "source": [
    "## 贝叶斯分类器\n",
    "贝叶斯决策论是概率框架下实施决策的基本方法，对于分类任务来说，在所有相关概率都已知的理想情况下，贝叶斯决策论考虑如何基于这些概率和误判损失来选择最优的类别标记。\n",
    "\n",
    "假设有N种可能的类别标记，即$Y ={c_1,c_2,c_3...c_N},\\lambda_{ij}$是将一个真实标记为$c_j$ 的样误分类为$c_i$所产生的损失。基于后验概率$P(c_i|x)$可以获得将样本x误分类成$c_i$所产生的的期望损失，记载样本$x$上的“条件风险” \n",
    "$$ R(c_i|x)=\\displaystyle\\sum_{j=1}^N \\lambda_{ij}P(c_i|x) $$\n",
    "\n",
    "我们的任务是寻找一个判定准则h:X<->Y以最小化总体风险 $$ R(h)=E_x[R(h(x)|x)] $$ 显然，对于每个样本$x$，若h能最小化条件风险$R(h(x)|x)$则总体风险$R(h)$也将被最小化，这就产生了贝叶斯判定准则：为最小化总体风险，只需在每个样本上选择那个能使条件风险$R(c|x)$最小的类别标记，即$$h(x) = argmin_{c \\in Y}R(c|x)$$ 此时，为贝叶斯最优分类器，与之对应的总体风险$R(h)$称为贝叶斯风险，$1-R(h)$反映了分类器所能达到的最好性能，即通过机器学习所能产生的模型精度的理论上限。 \n",
    "\n",
    "具体来说，若目标是最小化分类错误率，则误判损失$\\lambda_{ij}$可以表示为，当$i=j$时，$\\lambda_{ij}=0$，否则为1。 此时条件风险为 $$ R(c|x)=1-P(c|x) $$ ,于是，最小化分类错误率的贝叶斯最优分类器为 $$ h(x)= argmax_{c\\in Y}P(c|x) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08378284",
   "metadata": {},
   "source": [
    "### 朴素贝叶斯分类器\n",
    "朴素贝叶斯分类器采用了“属性条件独立性假设”：对已知类别，假设所有属性相互独立，换言之，假设每个属性独立地对分类结果发生影响。基于属性条件独立性假 $$ P(c|x)=\\frac{P(c)P(x|c)}{P(x)}=\\frac{P(c)}{P(x)}\\displaystyle\\prod_{i=1}^dP(x_i|c) $$ 由于对于所有类别来说P(x)相同，贝叶斯判定准则有 $$ h_{nb}(x)=argmax_{c∈Y}P(c)\\displaystyle\\prod_{i=1}^dP(c|x) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c5e475e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "import Seaborn as sns \n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2093b35",
   "metadata": {},
   "source": [
    "#### 生成随机数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e5e466e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "# make_blobs：为聚类产生数据集\n",
    "# n_samples：样本点数，n_features：数据的维度，centers:产生数据的中心点，默认值3\n",
    "# cluster_std：数据集的标准差，浮点数或者浮点数序列，默认值1.0，random_state：随机种子\n",
    "X, y = make_blobs(n_samples = 100, n_features = 2, centers = 2, random_state = 2, cluster_std = 1.5)\n",
    "plt.scatter(X[:,0], X[:,1], c=y, s=50, cmap='RdBu')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b9ed4c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "model = GaussianNB()\n",
    "model.fit(X, y)\n",
    "rng = np.random.RandomState(0)\n",
    "X_test = [-6, -14] + [14, 18] * rng.rand(2000, 2)\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "plt.scatter(X[:,0], X[:,1], c=y, s=50, cmap='RdBu')\n",
    "lim = plt.axis()\n",
    "plt.scatter(X_test[:,0], X_test[:,1], c=y_pred, s=20, cmap='RdBu', alpha = 0.1)\n",
    "plt.axis(lim)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec1347c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "model = GaussianNB()\n",
    "model.fit(X, y)\n",
    "rng = np.random.RandomState(0)\n",
    "X_test = [-6, -14] + [14, 18] * rng.rand(2000, 2)\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# 将训练集和数据集的数据用图像表示出来，颜色深直径大的为训练集，颜色浅直径小的为测试集\n",
    "plt.scatter(X[:,0], X[:,1], c=y, s=50, cmap='RdBu')\n",
    "lim = plt.axis()\n",
    "plt.scatter(X_test[:,0], X_test[:,1], c=y_pred, s=20, cmap='RdBu', alpha=0.1)\n",
    "plt.axis(lim)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a283013a",
   "metadata": {},
   "outputs": [],
   "source": [
    "yprob = model.predict_proba(X_test)\n",
    "yprob[-8:].round(2)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "448fe501",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'matplotlib'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mget_ipython\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_line_magic\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mmatplotlib\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43minline\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\IPython\\core\\interactiveshell.py:2504\u001b[39m, in \u001b[36mInteractiveShell.run_line_magic\u001b[39m\u001b[34m(self, magic_name, line, _stack_depth)\u001b[39m\n\u001b[32m   2502\u001b[39m     kwargs[\u001b[33m'\u001b[39m\u001b[33mlocal_ns\u001b[39m\u001b[33m'\u001b[39m] = \u001b[38;5;28mself\u001b[39m.get_local_scope(stack_depth)\n\u001b[32m   2503\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m.builtin_trap:\n\u001b[32m-> \u001b[39m\u001b[32m2504\u001b[39m     result = \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2506\u001b[39m \u001b[38;5;66;03m# The code below prevents the output from being displayed\u001b[39;00m\n\u001b[32m   2507\u001b[39m \u001b[38;5;66;03m# when using magics with decorator @output_can_be_silenced\u001b[39;00m\n\u001b[32m   2508\u001b[39m \u001b[38;5;66;03m# when the last Python token in the expression is a ';'.\u001b[39;00m\n\u001b[32m   2509\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(fn, magic.MAGIC_OUTPUT_CAN_BE_SILENCED, \u001b[38;5;28;01mFalse\u001b[39;00m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\IPython\\core\\magics\\pylab.py:103\u001b[39m, in \u001b[36mPylabMagics.matplotlib\u001b[39m\u001b[34m(self, line)\u001b[39m\n\u001b[32m     98\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[32m     99\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAvailable matplotlib backends: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    100\u001b[39m         % _list_matplotlib_backends_and_gui_loops()\n\u001b[32m    101\u001b[39m     )\n\u001b[32m    102\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m103\u001b[39m     gui, backend = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mshell\u001b[49m\u001b[43m.\u001b[49m\u001b[43menable_matplotlib\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgui\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    104\u001b[39m     \u001b[38;5;28mself\u001b[39m._show_matplotlib_backend(args.gui, backend)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\IPython\\core\\interactiveshell.py:3787\u001b[39m, in \u001b[36mInteractiveShell.enable_matplotlib\u001b[39m\u001b[34m(self, gui)\u001b[39m\n\u001b[32m   3784\u001b[39m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmatplotlib_inline\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbackend_inline\u001b[39;00m\n\u001b[32m   3786\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mIPython\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m pylabtools \u001b[38;5;28;01mas\u001b[39;00m pt\n\u001b[32m-> \u001b[39m\u001b[32m3787\u001b[39m gui, backend = \u001b[43mpt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfind_gui_and_backend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgui\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpylab_gui_select\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3789\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m gui != \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   3790\u001b[39m     \u001b[38;5;66;03m# If we have our first gui selection, store it\u001b[39;00m\n\u001b[32m   3791\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.pylab_gui_select \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\IPython\\core\\pylabtools.py:338\u001b[39m, in \u001b[36mfind_gui_and_backend\u001b[39m\u001b[34m(gui, gui_select)\u001b[39m\n\u001b[32m    321\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfind_gui_and_backend\u001b[39m(gui=\u001b[38;5;28;01mNone\u001b[39;00m, gui_select=\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m    322\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Given a gui string return the gui and mpl backend.\u001b[39;00m\n\u001b[32m    323\u001b[39m \n\u001b[32m    324\u001b[39m \u001b[33;03m    Parameters\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    335\u001b[39m \u001b[33;03m    'WXAgg','Qt4Agg','module://matplotlib_inline.backend_inline','agg').\u001b[39;00m\n\u001b[32m    336\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m338\u001b[39m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmatplotlib\u001b[39;00m\n\u001b[32m    340\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m _matplotlib_manages_backends():\n\u001b[32m    341\u001b[39m         backend_registry = matplotlib.backends.registry.backend_registry\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'matplotlib'"
     ]
    }
   ],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a4563b4",
   "metadata": {},
   "source": [
    "#### 深度学习基础及数学原理\n",
    "\n",
    "##### 监督学习和无监督学习\n",
    "\n",
    "监督学习、无监督学习、半监督学习、强化学习是我们日常接触到的常见的四个机器学习方法：\n",
    "- 监督学习：通过已有的训练样本（即已知数据以及其对应的输出）去训练得到一个最优模型（这个模型属于某个函数的集合，最优则表示在某个评价准则下是最佳的），再利用这个模型将所有的输入映射为相应的输出。\n",
    "- 无监督学习：它与监督学习的不同之处，在于我们事先没有任何训练样本，而需要直接对数据进行建模。\n",
    "- 半监督学习 ：在训练阶段结合了大量未标记的数据和少量标签数据。与使用所有标签数据的模型相比，使用训练集的训练模型在训练时可以更为准确。\n",
    "- 强化学习：我们设定一个回报函数（reward function），通过这个函数来确认否越来越接近目标，类似我们训练宠物，如果做对了就给他奖励，做错了就给予惩罚，最后来达到我们的训练目的。\n",
    "  \n",
    "##### 线性回归\n",
    "\n",
    "线性回归是利用数理统计中回归分析，来确定两种或两种以上变量间相互依赖的定量关系的一种统计分析方法，运用十分广泛。其表达形式为y = w'x+e，e为误差服从均值为0的正态分布。\n",
    "\n",
    "线性回归对于输入x与输出y有一个映射f，y=f(x),而f的形式为aX+b。其中a和b是两个可调的参数，我们训练的时候就是训练a，b这两个参数。\n",
    "\n",
    "下面我们来用PyTorch的代码来做一个详细的解释："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d0449fbe",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mnn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Linear, Module, MSELoss\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01moptim\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SGD\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.nn import Linear, Module, MSELoss\n",
    "from torch.optim import SGD\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88e647e3",
   "metadata": {},
   "source": [
    "下面定义一个线性函数，这里使用 $y = 5x + 7 $，这里的5和7就是上面说到的参数a和b，我们先使用matplot可视化一下这个函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2ca97a9c",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m x = \u001b[43mnp\u001b[49m.linspace(\u001b[32m0\u001b[39m, \u001b[32m20\u001b[39m, \u001b[32m500\u001b[39m)\n\u001b[32m      2\u001b[39m y = \u001b[32m5\u001b[39m * x + \u001b[32m7\u001b[39m\n\u001b[32m      3\u001b[39m plt.plot(x, y)\n",
      "\u001b[31mNameError\u001b[39m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "x = np.linspace(0, 20, 500)\n",
    "y = 5 * x + 7\n",
    "plt.plot(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "715ebdb8",
   "metadata": {},
   "source": [
    "下面生成一些随机的点，来作为训练数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "044b4e36",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m x = \u001b[43mnp\u001b[49m.random.rand(\u001b[32m256\u001b[39m)\n\u001b[32m      2\u001b[39m noise = np.random.randn(\u001b[32m256\u001b[39m) / \u001b[32m4\u001b[39m\n\u001b[32m      3\u001b[39m y = \u001b[32m5\u001b[39m * x + \u001b[32m7\u001b[39m + noise\n",
      "\u001b[31mNameError\u001b[39m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "x = np.random.rand(256)\n",
    "noise = np.random.randn(256) / 4\n",
    "y = 5 * x + 7 + noise\n",
    "df = pd.DataFrame()\n",
    "df['x'] = x\n",
    "df['y'] = y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "958ef76c",
   "metadata": {},
   "source": [
    "在图上显示下我们生成的数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a1a7b63",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lmplot(x='x', y='y', data=df, height=6, aspect=1.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71fa7649",
   "metadata": {},
   "source": [
    "我们随机生成了一些点，下面将使用PyTorch建立一个线性的模型来对其进行拟合，这就是所说的训练的过程，由于只有一层线性模型，所以我们就直接使用了"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ced90c90",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Linear' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m model = \u001b[43mLinear\u001b[49m(\u001b[32m1\u001b[39m, \u001b[32m1\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'Linear' is not defined"
     ]
    }
   ],
   "source": [
    "model = Linear(1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3a4ad39",
   "metadata": {},
   "source": [
    "其中参数(1, 1)代表输入输出的特征(feature)数量都是1。`Linear`模型的表达式是 $y=wx+b$，其中其中w代表权重，b代表偏置\n",
    "\n",
    "损失函数我们使用均方损失函数：`MSELoss`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e35c2f5d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'MSELoss' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m criterion = \u001b[43mMSELoss\u001b[49m()\n",
      "\u001b[31mNameError\u001b[39m: name 'MSELoss' is not defined"
     ]
    }
   ],
   "source": [
    "criterion = MSELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e310eee",
   "metadata": {},
   "source": [
    "优化器我们选择最常见的优化方法`SGD`，就是每一次迭代计算`mini-batch`的梯度，然后对参数进行更新，学习率 0.01 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8e6e6e8b",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'SGD' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m optim = \u001b[43mSGD\u001b[49m(model.parameters(), lr = \u001b[32m0.01\u001b[39m)\n\u001b[32m      2\u001b[39m epochs = \u001b[32m3000\u001b[39m\n",
      "\u001b[31mNameError\u001b[39m: name 'SGD' is not defined"
     ]
    }
   ],
   "source": [
    "optim = SGD(model.parameters(), lr = 0.01)\n",
    "epochs = 3000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32d6b7e3",
   "metadata": {},
   "source": [
    "准备训练数据：`x_train`,`y_train`的形状是 (256, 1)， 代表`mini-batch`大小为256，`feature`为1。\n",
    "`astype('float32')`是为了下一步可以直接转换为`torch.float`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4963eb82",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'x' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m x_train = \u001b[43mx\u001b[49m.reshape(-\u001b[32m1\u001b[39m, \u001b[32m1\u001b[39m).astype(\u001b[33m'\u001b[39m\u001b[33mfloat32\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m      2\u001b[39m y_train = y.reshape(-\u001b[32m1\u001b[39m, \u001b[32m1\u001b[39m).astype(\u001b[33m'\u001b[39m\u001b[33mfloat32\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'x' is not defined"
     ]
    }
   ],
   "source": [
    "x_train = x.reshape(-1, 1).astype('float32')\n",
    "y_train = y.reshape(-1, 1).astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "27f23106",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'epochs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[43mepochs\u001b[49m):\n\u001b[32m      2\u001b[39m     \u001b[38;5;66;03m# 整理输入和输出的数据，这里输入和输出一定要是torch的Tensor类型\u001b[39;00m\n\u001b[32m      3\u001b[39m     inputs = torch.from_numpy(x_train)\n\u001b[32m      4\u001b[39m     labels = torch.from_numpy(y_train)\n",
      "\u001b[31mNameError\u001b[39m: name 'epochs' is not defined"
     ]
    }
   ],
   "source": [
    "for i in range(epochs):\n",
    "    # 整理输入和输出的数据，这里输入和输出一定要是torch的Tensor类型\n",
    "    inputs = torch.from_numpy(x_train)\n",
    "    labels = torch.from_numpy(y_train)\n",
    "\n",
    "    # 使用模型进行预测\n",
    "    outputs = model(inputs)\n",
    "\n",
    "    # 梯度置0，否则会累加\n",
    "    optim.zero_grad()\n",
    "\n",
    "    # 计算损失\n",
    "    loss = criterion(outputs, labels)\n",
    "\n",
    "    # 反向传播\n",
    "    loss.backward()\n",
    "\n",
    "    # 使用优化器默认方法优化\n",
    "    optim.step()\n",
    "    if (i % 100 == 0):\n",
    "        print('epoch{}, loss{: 1.4f}'.format(i, loss.data.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97651d66",
   "metadata": {},
   "outputs": [],
   "source": [
    "[w, b] = model.parameters()  #用model.parameters()提取模型参数。\n",
    "print(w.item(), b.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d21bc1d6",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# 可视化模型\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m predicted = \u001b[43mmodel\u001b[49m.forward(torch.from_numpy(x_train)).data.numpy()\n\u001b[32m      3\u001b[39m plt.plot(x_train, y_train, \u001b[33m'\u001b[39m\u001b[33mgo\u001b[39m\u001b[33m'\u001b[39m, label = \u001b[33m'\u001b[39m\u001b[33mdata\u001b[39m\u001b[33m'\u001b[39m, alpha = \u001b[32m0.3\u001b[39m)\n\u001b[32m      4\u001b[39m plt.plot(x_train, predicted, label = \u001b[33m'\u001b[39m\u001b[33mpredicted\u001b[39m\u001b[33m'\u001b[39m, alpha = \u001b[32m1\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "# 可视化模型\n",
    "predicted = model.forward(torch.from_numpy(x_train)).data.numpy()\n",
    "plt.plot(x_train, y_train, 'go', label = 'data', alpha = 0.3)\n",
    "plt.plot(x_train, predicted, label = 'predicted', alpha = 1)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73402a73",
   "metadata": {},
   "source": [
    "#### 损失函数（Loss Function）\n",
    "\n",
    "损失函数是用来估量模型的预测值与真实值的不一致程度，它是一个非负实值函数，损失函数越小，模型的稳健性越好。\n",
    "\n",
    "我们训练模型的过程，就是通过不断地迭代计算，使用梯度下降地优化算法，使得损失函数越来越小。损失函数越小就表示算法达到意义上的最优。\n",
    "\n",
    "这里有一个重点：因为PyTorch是使用mini-batch来进行计算的，所以损失函数的计算出来的结果已经对mini-batch取了平均\n",
    "\n",
    "常见（PyTorch内置）的损失函数有以下几个：\n",
    "- `nn.L1Loss`:输入x和目标y之间差的绝对值，要求x和y的维度一样（可以是向量或矩阵），得到的loss维度也是对应一样的。 $$loss(x, y) = 1/n \\sum{|x_i - y_i|}$$\n",
    "- `nn.NLLoss`:用于多分类的负对数似然函数损失值。 $$loss(x, class) = -x[class]$$ 如果传递了weights参数，会对损失进行加权，公式就变成了 $$ loss(x, class) = - weights[class] * x[class] $$\n",
    "- `nn.MSELoss`:均方损失函数，输入x和目标y之间的均方差。 $$loss(x, y) = 1/n \\sum{(x_i - y_i)^2}$$\n",
    "- `nn.CrossEntropyLoss`:多分类用的交叉熵损失函数，`LogSoftMax`和`NLLLoss`集成到一个类中，会调用`nn.NLLLoss`函数，我们可以理解为 $CrossEntropyLoss()=logsoftmax() + NLLLoss()$   $$loss(x, class) = -log\\frac{exp(x[class])}{\\sum_j{exp(x[j])}} = -x[class] + log(\\sum_j{exp(x[j])})$$   因为使用了`NLLLoss`，所以也可以传入weight参数，这时loss的计算公式变为：$$loss(x, class) = weights[class] * (-x[class] + log(\\sum_j{exp(x[j])}))$$ 所以一般多分类情况会用这个损失函数\n",
    "- `nn.BCELoss`:计算$x$与$y$之间的二进制交叉熵。 $$loss(o, t) = -\\frac{1}{n} \\sum_i{(t[i] * log(o[i]) + (1-t[i])* log(1 - o[i]))}$$  与`NLLLoss`类似，也可以添加权重参数 $$loss(o, t) = -\\frac{1}{n} \\sum_i{weights[i]*((t[i] * log(o[i]) + (1-t[i])* log(1 - o[i])))}$$  用的时候需要在该层前面加上Sigmoid函数。\n",
    "\n",
    "#### 梯度下降\n",
    "梯度下降是一个使损失函数越来越小的优化算法，在求解机器学习算法的模型参数，即约束优化问题时，梯度下降（Gradient Descent）是最常采用的方法之一。\n",
    "\n",
    "##### 梯度\n",
    "在微积分里面，对多元参数的求偏导数，把求得的各个参数的偏导数以向量的形式写出来，就是梯度。例如函数$f(x,y)$，分别对x，y求偏导数，求得的梯度向量就是$\\left( \\frac{\\partial f}{\\partial x}, \\frac{\\partial f}{\\partial y} \\right)^{\\!\\top}$，简称 $\\operatorname{grad} f(x,y)$ 或 $\\nabla f(x,y)$。\n",
    "\n",
    "几何上讲，梯度就是函数变化增加最快的地方，沿着梯度向量的方向，更加容易找到函数的最大值。反过来说，沿着梯度向量相反的方向梯度减少最快，也就是更加容易找到最小值。\n",
    "\n",
    "##### Mini-batch的梯度下降法\n",
    "对整个训练集进行梯度下降法的时候，我们必须处理整个训练数据集，然后才能进行一步梯度下降，即每一步梯度下降法需要对整个训练集进行一次处理，如果训练数据集很大的时候处理速度会很慢，而且也不可能一次的载入到内存或者显存中，所以我们会把大数据集分成小数据集，一部分一部分的训练，这个训练子集即称为Mini-batch。\n",
    "\n",
    "对于普通的梯度下降法，一个epoch只能进行一次梯度下降；而对于Mini-batch梯度下降法，一个epoch可以进行Mini-batch的个数次梯度下降。\n",
    "\n",
    "- 如果训练样本的大小比较小时，能够一次性的读取到内存中，那我们就不需要使用Mini-batch，\n",
    "- 如果训练样本的大小比较大时，一次读入不到内存或者现存中，那我们必须要使用 Mini-batch来分批的计算\n",
    "- Mini-batch size的计算规则如下，在内存允许的最大情况下使用2的N次方个size\n",
    "\n",
    "`torch.optim`是一个实现了各种优化算法的库。大部分常用优化算法都有实现，我们直接调用即可。\n",
    "\n",
    "- `troch.optim.SGD`:随机梯度下降算法，带有动量（momentum）的算法，作为一个可选参数，可以进行设置。样例如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cf2a8162",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m optimizer = \u001b[43mtorch\u001b[49m.optim.SGD(model.parameters(), lr = \u001b[32m0.1\u001b[39m, momentum= \u001b[32m0.9\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.SGD(model.parameters(), lr = 0.1, momentum= 0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd4b0d1d",
   "metadata": {},
   "source": [
    "- `torch.optim.RMSprop`:除了以上的带有动量Momentum梯度下降算法外，RMSprop（root mean square prop）也是一种可以加快梯度下降的算法，利用RMSprop算法，可以减小某些维度梯度更新波动较大的情况，使其梯度下降的速度变得更快"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3dc73b79",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m optimizer = \u001b[43mtorch\u001b[49m.optim.RMSprop(model.parameters(), lr = \u001b[32m0.01\u001b[39m, alpha = \u001b[32m0.99\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.RMSprop(model.parameters(), lr = 0.01, alpha = 0.99)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46847e1c",
   "metadata": {},
   "source": [
    "- `torch.optim.Adam`:Adam 优化算法的基本思想就是将 Momentum 和 RMSprop 结合起来形成的一种适用于不同深度学习结构的优化算法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9b31df15",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m optimizer = \u001b[43mtorch\u001b[49m.optim.Adam(model.parameters(), lr = \u001b[32m0.001\u001b[39m, betas = (\u001b[32m0.9\u001b[39m, \u001b[32m0.999\u001b[39m), eps = \u001b[32m1e-08\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr = 0.001, betas = (0.9, 0.999), eps = 1e-08)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5137b10e",
   "metadata": {},
   "source": [
    "#### 方差/偏差\n",
    "- 偏差度量了学习算法的期望预测与真实结果的偏离程序，即刻画了学习算法本身的拟合能力\n",
    "- 方差度量了同样大小的训练集的变动所导致的学习性能的变化，即模型的泛化能力"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4b9bf15",
   "metadata": {},
   "source": [
    "高偏差（high bias）的情况，一般称为欠拟合（underfitting），即我们的模型并没有很好的去适配现有的数据，拟合度不够。\n",
    "\n",
    "高方差（high variance）的情况一般称作过拟合（overfitting），即模型对于训练数据拟合度太高了，失去了泛化的能力。\n",
    "\n",
    "欠拟合：\n",
    "- 增加网络结构，如增加隐藏层数目；\n",
    "- 训练更长时间；\n",
    "- 寻找合适的网络架构，使用更大的NN结构；\n",
    "\n",
    "过拟合:\n",
    "- 使用更多的数据；\n",
    "- 正则化（regularization）；\n",
    "- 寻找合适的网络结构；\n",
    "\n",
    "例如我们上面的例子，可以计算出我们的偏差:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cb865c47",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'w' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[32m5\u001b[39m-\u001b[43mw\u001b[49m.data.item(), \u001b[32m7\u001b[39m-b.data.ietm())\n",
      "\u001b[31mNameError\u001b[39m: name 'w' is not defined"
     ]
    }
   ],
   "source": [
    "print(5-w.data.item(), 7-b.data.ietm())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b8254b1",
   "metadata": {},
   "source": [
    "#### 正则化\n",
    "利用正则化来解决High variance 的问题，正则化是在 Cost function 中加入一项正则化项，惩罚模型的复杂度。\n",
    "\n",
    "- L1正则化：损失函数基础上加上权重参数的绝对值 $L = E _{in} + \\lambda\\sum_j{|w_j|}$\n",
    "- L2正则化：损失函数基础上加上权重参数的平方和 $L = E _{in} + \\lambda\\sum_j{w_j^2}$\n",
    "\n",
    "需要说明的是：l1 相比于 l2 会更容易获得稀疏解\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d58b767",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

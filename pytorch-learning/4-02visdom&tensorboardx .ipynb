{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d72002f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import math\n",
    "import nunmpy as np\n",
    "from visdom import Visdom\n",
    "import time\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c9686a4",
   "metadata": {},
   "source": [
    "### 使用Visdom在`PyTorch`中进行可视化\n",
    "\n",
    "#### 安装\n",
    "\n",
    "`pip install visdom`安装\n",
    "\n",
    "`python -m visdom.server`在本地启动服务器,启动后会提示`It's Alive! You can navigate to http://localhost:8097`,我们打开浏览器，输入`http://localhost:8097`即可看到页面。\n",
    "\n",
    "端口8097是默认的端口可以在启动命令后加`-port`参数指定端口，常用的参数还有`--hostname`,`-base_url`等。\n",
    "\n",
    "#### 坑\n",
    "Visdom的服务在启动时会自动下载一些静态文件，因为某些无法描述的原因，导致下载会失败，比如类似这样的提示`ERROR:root:Error 404 while downloading https://unpkg.com/layout-bin-packer@1.4.0`,就说明静态文件没有下载完全，这样有可能就会打不开或者页面中没有菜单栏，那么需要手动进行下载。\n",
    "\n",
    "如果不知道conda的环境目录在哪里，可以使用`conda env list`查看。\n",
    "\n",
    "### 基本概念\n",
    "#### Environments\n",
    "Environments的作用是对可视化区域进行分区，每个用户都会有一个叫做`main`的默认分区。在程序指定的情况下，默认的图表都会放到这里面。\n",
    "\n",
    "#### Panes\n",
    "`Panes`作为每一个可视化图表的容器，可以使用生成的图表，图片，文本进行填充，我们可以对`Panes`进行拖放，删除，调整大小和销毁等操作。\n",
    "\n",
    "Panes和Environments是一对多的关系，即一个Environments可以包含多个Panes。\n",
    "\n",
    "#### VIEW\n",
    "在对Panes进行调整后，可以通过VIEW对状态进行管理：\n",
    "\n",
    "#### 可视化接口\n",
    "Visdom是由Plotly提供的可视化支持，所以提供以下可视化的接口:\n",
    "\n",
    "- `vis.scatter`:2D 或 3D 散点图\n",
    "- `vis.line`:线图\n",
    "- `vis.stem`:茎叶图\n",
    "- `vis.heatmap`:热力图\n",
    "- `vis.bar`:条形图\n",
    "- `vis.histogram`:直方图\n",
    "- `vis.boxplot`: 箱型图\n",
    "- `vis.surf`: 表面图\n",
    "- `vis.contour`: 轮廓图\n",
    "- `vis.quiver`: 绘出二维矢量场\n",
    "- `vis.image`: 图片\n",
    "- `vis.text`: 文本\n",
    "- `vis.mesh`: 网格图\n",
    "- `vis.save`: 序列化状态\n",
    "\n",
    "### 使用\n",
    "#### 绘制简单的图形\n",
    "\n",
    "这里我们使用官方的DEMO来做样例\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1345383",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = Visdom()\n",
    "assert env.check_connection()  #测试一下链接，链接错误的话会报错\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52965988",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 这里生成sin和cos两条曲线数据\n",
    "Y = np.linspace(0, 2*math.pi, 70)\n",
    "X = np.column_stack((np.sin(Y), np.cos(Y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29d637d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用茎叶图展示\n",
    "env.stem(\n",
    "    X = X,\n",
    "    Y = Y,\n",
    "    opts = dict(legend = ['Sine', 'Cosine'])\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef5f3fdb",
   "metadata": {},
   "source": [
    "可以通过env参数指定Environments，如果名称包含了下划线_那么visdom会跟根据下划线分割并自动分组"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "608855d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "envtest = Visdom(env = 'test_mesh')\n",
    "assert envtest.check_connection()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "039e305d",
   "metadata": {},
   "source": [
    "生成一个网格图"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e4f5e0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [0, 0, 1, 1, 0, 0, 1, 1]\n",
    "y = [0, 1, 1, 0, 0, 1, 1, 0]\n",
    "z = [0, 0, 0, 0, 1, 1, 1, 1]\n",
    "\n",
    "X = np.c_[x, y, z]  # 将x,y,z组合成一个矩阵\n",
    "i = [7, 0, 0, 0, 4, 4, 6, 6, 4, 0, 3, 2]\n",
    "j = [3, 4, 1, 2, 5, 6, 5, 2, 0, 1, 6, 3]\n",
    "k = [0, 7, 2, 3, 6, 7, 1, 1, 5, 5, 7, 6]\n",
    "Y = np.c_[i, j, k]\n",
    "\n",
    "envtest.mesh(\n",
    "    X = X,\n",
    "    Y = Y,\n",
    "    opts = dict(opacity = 0.5)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aed2c32d",
   "metadata": {},
   "source": [
    "### 更新损失函数\n",
    "在训练的时候我们每一批次都会打印一下训练的损失和测试的准确率，这样展示的图表是需要动态增加数据的，下面我们来模拟一下这种情况"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6396f939",
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = 0, 0\n",
    "env2 = Visdom()\n",
    "pane1 = env2.line(\n",
    "    X = np.array([x]),\n",
    "    Y = np.array([y]),\n",
    "    opts = dict(\n",
    "        title = 'dynamic data'\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19454090",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    time.sleep(1) # 每隔一秒钟打印一次数据\n",
    "    x += 1\n",
    "    y = (y + i) * 1.5\n",
    "    print(x, y)\n",
    "    env2.line(\n",
    "        X = np.array([x]),\n",
    "        Y = np.array([y]),\n",
    "        win = pane1,\n",
    "        update = 'append'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53209601",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "from torchvisison import models, datasets\n",
    "torch.__version__\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6c19502",
   "metadata": {},
   "source": [
    "### 使用Tensorboard在 PyTorch 中进行可视化\n",
    "\n",
    "#### Tensorboard 简介\n",
    "\n",
    "Tensorboard是tensorflow内置的一个可视化工具，它通过将tensorflow程序输出的日志文件的信息可视化使得tensorflow程序的理解、调试和优化更加简单高效。\n",
    "\n",
    "Tensorboard的可视化依赖于tensorflow程序运行输出的日志文件，因而tensorboard和tensorflow程序在不同的进程中运行。 \n",
    "\n",
    "tensorboard虽然是tensorflow内置的可视化工具，但是他们跑在不同的进程中，所以Github上已经有大神将tensorboard应用到Pytorch中。\n",
    "\n",
    "\n",
    "#### Tensorboard 安装\n",
    "\n",
    "首先需要安装tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c161c080",
   "metadata": {
    "vscode": {
     "languageId": "bat"
    }
   },
   "outputs": [],
   "source": [
    "pip install tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a10dc951",
   "metadata": {},
   "source": [
    "然后再安装tensorboardx "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4d63983",
   "metadata": {
    "vscode": {
     "languageId": "bat"
    }
   },
   "outputs": [],
   "source": [
    "pip install tensorboardx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aeae310",
   "metadata": {},
   "source": [
    "pytorch 1.1以后的版本内置了SummaryWriter 函数,所以不需要再安装tensorboardx了。\n",
    "\n",
    "安装完成后与`visdom`一样执行独立的命令` tensorboard --logdir logs` 即可启动，默认的端口是 6006,在浏览器中打开 `http://localhost:6006/` 即可看到web页面。\n",
    "\n",
    "这里要说明的是 微软的Edge浏览器css会无法加载，使用chrome正常显示。\n",
    "\n",
    "### 页面\n",
    "与visdom不同，tensorboard针对不同的类型人为的区分多个标签，每一个标签页面代表不同的类型。 \n",
    "\n",
    "- SCALAR\n",
    "\n",
    "对标量数据进行汇总和记录，通常用来可视化训练过程中随着迭代次数准确率(val acc)、损失值(train/test loss)、学习率(learning rate)、每一层的权重和偏置的统计量(mean、std、max/min)等的变化曲线。\n",
    "\n",
    "- IMAGES\n",
    "\n",
    "可视化当前轮训练使用的训练/测试图片或者 feature maps\n",
    "\n",
    "- GRAPHS\n",
    "\n",
    "可视化计算图的结构及计算图上的信息，通常用来展示网络的结构\n",
    "\n",
    "- HISTOGRAMS\n",
    "\n",
    "可视化张量的取值分布，记录变量的直方图（统计张量随着迭代轮数的变化情况）\n",
    "\n",
    "- PROJECTOR\n",
    "\n",
    "全称Embedding Projector高维向量进行可视化\n",
    "\n",
    "### 使用\n",
    "在使用前请先去确认执行`tensorboard --logdir logs`并保证`http://localhost:6006/`页面能够正常打开\n",
    "\n",
    "#### 图像展示\n",
    "\n",
    "首先介绍比较简单的功能，查看我们训练集和数据集中的图像，这里我们使用现成的图像作为展示。这里使用wikipedia上的一张猫的图片"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2afaa3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#引入 tensorboardX 包\n",
    "#  这里的引用也要修改成torch的引用\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "907a027c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_img = Image.open('./....')\n",
    "cat_img.size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f30167f",
   "metadata": {},
   "source": [
    "这是一张1280x853的图，我们先把她变成224x224的图片，因为后面要使用的是vgg16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d76a2e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_224 = transforms.Compose([\n",
    "    transforms.Resize(224),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "cat_img_224 = transform_224(cat_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "179bfbf9",
   "metadata": {},
   "source": [
    "将图片展示在tebsorboard中："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e2adb5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = SummaryWriter(log_dir = './logs', comment = 'cat image')  # 这里的logs要与--logdir的参数一样\n",
    "writer.add_image(\"cat\",cat_img_224)\n",
    "writer.close()# 执行close立即刷新，否则将每120秒自动刷新"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b929368",
   "metadata": {},
   "source": [
    "浏览器访问[[http://localhost:6006/#images]]即可看到猫的图片"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49b00712",
   "metadata": {},
   "source": [
    "#### 更新损失函数\n",
    "更新损失函数和训练批次我们与visdom一样使用模拟展示，这里用到的是tensorboard的SCALAR页面"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24002dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.FloatTensor([100])\n",
    "y = torch.Floattensor([500])\n",
    "\n",
    "for epoch in range(30):\n",
    "    x = x * 1.2\n",
    "    y = y / 1.1\n",
    "    loss = np.random.random()\n",
    "    with SummaryWriter(log_dir = './logs', comment = 'train') as writer:\n",
    "        writer.add_histogram('his/x', x, epoch)\n",
    "        writer.add_histogram('his/y', y, epoch)\n",
    "        writer.add_scalar('data/x', x, epoch)\n",
    "        writer.add_scalar('data/y', y, epoch)\n",
    "        writer.add_scalar('data/loss', loss, epoch)\n",
    "        writer.add_sclars('data/data_group', {'x': x, 'y': y}, epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e9c10e1",
   "metadata": {},
   "source": [
    "浏览器访问[http://localhost:6006/#scalars]即可看到图形"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56226f04",
   "metadata": {},
   "source": [
    "#### 使用PROJECTOR对高维向量可视化\n",
    "\n",
    "PROJECTOR的的原理是通过PCA，T-SNE等方法将高维向量投影到三维坐标系（降维度）。Embedding Projector从模型运行过程中保存的checkpoint文件中读取数据，默认使用主成分分析法（PCA）将高维数据投影到3D空间中，也可以通过设置设置选择T-SNE投影方法，这里做一个简单的展示。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42300355",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 512\n",
    "EPOCHS = 20\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('data', train = True, download = True,\n",
    "                   transform = transforms.Compose([\n",
    "                    transforms.ToTensor(),\n",
    "                    transforms.Normalize((0.137,), (0.3081,))\n",
    "                   ])),\n",
    "    batch_size = BATCH_SIZE, shuffle = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b438673f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # 1,28x28\n",
    "        self.conv1 = nn.Conv2d(1, 10, 5)\n",
    "        self.conv2 = nn.Conv2d(10, 20, 3)\n",
    "        self.fc1 = nn.Linear(20*10*10, 500)\n",
    "        self.fc2 = nn.Linear(500, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        in_size = x.size(0)\n",
    "        out = self.conv1(x)\n",
    "        out = F.relu(out)\n",
    "        out = F.max_pool2d(out, 2, 2)\n",
    "        out = self.conv2(out)\n",
    "        out = F.relu(out)\n",
    "        out = out.view(in_size, -1)\n",
    "        out = self.fc1(out)\n",
    "        out = F.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        out = F.log_softmax(out, dim = 1)\n",
    "        return out\n",
    "\n",
    "model = ConvNet()\n",
    "optimizer = torch.optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a5f03a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, optimizer, epoch):\n",
    "    n_iter = 0\n",
    "    model.train()\n",
    "\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if (batch_idx + 1) % 30 == 0:\n",
    "            n_iter = n_iter + 1\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))\n",
    "\n",
    "            # 相较于以前的训练方法，主要增加了如下内容\n",
    "            out = torch.cat((output.data, torch.ones(len(output), 1)), 1)\n",
    "            with SummaryWriter(log_dir='./logs', comment='mnist') as writer: \n",
    "                #使用add_embedding方法进行可视化展示\n",
    "                writer.add_embedding(\n",
    "                    out,\n",
    "                    metadata = target.data,\n",
    "                    label_img = data.data,\n",
    "                    global_step = n_iter\n",
    "                )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cce9a07",
   "metadata": {},
   "source": [
    "这里节省时间，只训练一次"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04f8c7a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train(model, train_loader, optimizer, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7abd8351",
   "metadata": {},
   "source": [
    "打开[http://localhost:6006/#projector]即可看到效果。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43f1a62c",
   "metadata": {},
   "source": [
    "#### 绘制网络结构\n",
    "在pytorch中我们可以使用print直接打印出网络的结构，但是这种方法可视化效果不好，这里使用tensorboard的GRAPHS来实现网络结构的可视化。 由于pytorch使用的是动态图计算，所以我们这里要手动进行一次前向的传播。\n",
    "\n",
    "使用Pytorch已经构建好的模型进行展示\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0fd7a85",
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg16 = models.vgg16(pretrained = True)\n",
    "print(vgg16)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "783c538f",
   "metadata": {},
   "source": [
    "在前向传播前，先要把图片做一些调整"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89e6a35b",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_2 = transforms.Compose([\n",
    "    transforms.Resize(224),\n",
    "    transforms.CenterCrop((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean = [0.485, 0.456, 0.406], std = [0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d3d7292",
   "metadata": {},
   "source": [
    "使用上一张猫的图片进行前向传播"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b706240",
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg16_input = transform_2(cat_img)[np.newaxis] # 因为pytorch的是分批次进行的，所以我们这里建立一个批次为1的数据集\n",
    "vgg16_input.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7958d88",
   "metadata": {},
   "source": [
    "开始前向传播，打印输出值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "216c3e6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = vgg16(vgg16_input)\n",
    "_, preds = torch.max(out.data, 1)\n",
    "label = preds.numpy()[0]\n",
    "label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f47d8aeb",
   "metadata": {},
   "source": [
    "将结构图在tensorboard进行展示"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27289c53",
   "metadata": {},
   "outputs": [],
   "source": [
    "with SummaryWriter(log_dir = './logs', comment = 'vgg161') as writer:\n",
    "    writer.add_graph(vgg16, vgg16_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d82e0144",
   "metadata": {},
   "source": [
    "对于Pytorch的1.3版本来说，实测 SummaryWriter在处理结构图的时候是有问题的（或者是需要加什么参数，目前我还没找到），所以建议大家继续使用tensorboardx。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecc587ae",
   "metadata": {},
   "source": [
    "### 可视化理解卷积神经网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80f57517",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "369c7788",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_img=Image.open('./1280px-Felis_silvestris_catus_lying_on_rice_straw.jpg')\n",
    "transform_224= transforms.Compose([\n",
    "    transforms.Resize(224), \n",
    "    transforms.CenterCrop((224,224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "\n",
    "cat_img_224=transform_224(cat_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b8cde93",
   "metadata": {},
   "source": [
    "上面的代码是我们读取了一张图片，并对图片进行了一些预处理，下面我们来创建vgg16的预训练好网络模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dd9bf42",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = models.vgg16(pretrained = True)\n",
    "inputs = cat_img_224[np.newaxis] #这两个方法都可以cat_img_224[None,::]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71cdc812",
   "metadata": {},
   "source": [
    "进行一次前向的传播，看看得到了什么结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37f3097d",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = net(inputs)\n",
    "_, preds = torch.max(out.data, 1)\n",
    "preds\n",
    "label = preds.numpy()[0]\n",
    "label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "486290bf",
   "metadata": {},
   "source": [
    "不同的预训练权重也会出现不同的结果"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaedda9c",
   "metadata": {},
   "source": [
    "#### 卷积神经网络的可视化背景\n",
    "CNN模型虽然在图像处理上表现出非常良好的性能和准确性，但一直以来都被认为是一个黑盒模型，人们无法了解里面的工作机制。 针对这个问题，研究人员除了从理论层面去寻找解释外，也提出了一些可视化的方法直观地理解CNN的内部机理。\n",
    "\n",
    "#### 基于Deconvolution的方法\n",
    "Visualizing and Understanding Convolutional Networks 主要是将激活函数的特征映射回像素空间，来揭示什么样的输入模式能够产生特定的输出,因为网络是有层级关系的，所以越靠近输出的层级学到的特征越抽象，与实际任务越相关\n",
    "\n",
    "#### 基于Backpropagation的方法\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b0c6b82",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, './src/')\n",
    "\n",
    "def rgb2gray(rgb):\n",
    "    return np.dot(rgb[...,:3], [0.299, 0.587, 0.114])\n",
    "\n",
    "def rescale_grads(map, gradtype = \"all\"):\n",
    "    if(gradtype == \"pos\"):\n",
    "        map = (np.maximum(0, map) / map.max())\n",
    "    elif gradtype == \"neg\":\n",
    "        map = (np.maximum(0, -map)/ -map.min())\n",
    "    else:\n",
    "        map = map - map.min()\n",
    "        map /= map.max()\n",
    "\n",
    "    return map"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8461be3",
   "metadata": {},
   "source": [
    "#### Guided-Backpropagation\n",
    "大致的方法为：选择某一种输出模式，然后通过反向传播计算输出对输入的梯度。这种方式与上一种deconvnet的方式的唯一区别在于对ReLU梯度的处理。\n",
    "\n",
    "ReLU在反向传播的计算采用的前向传播的特征作为门阀，而deconvnet采用的是梯度值，guided-backpropagation则将两者组合在一起使用，这样有助于得到的重构都是正数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a05a0c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs.requires_grad = True   # 这句话必须要有，否则会报错\n",
    "from guided_backprop import GuidedBackprop    #这里直接引用写好的方法，在src，目录找想对应的文件\n",
    "GB = GuidedBackprop(net)\n",
    "gp_grads = GB.generate_gradients(inputs, label)\n",
    "gp_grads = np.moveaxis(gp.grads, 0, -1)\n",
    "\n",
    "# 分别计算三类gp\n",
    "ag = rescale_grads(gp_grads, gradtype = \"all\")\n",
    "pg = rescale_grads(gp_grads, gradtype = \"pos\")\n",
    "ng = rescale_grads(gp_grads, gradtype = \"neg\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc5a0522",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用matplotlib查看结果\n",
    "plt.imshow(cat_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72580804",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(ag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe811972",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(ng)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "772d7aaf",
   "metadata": {},
   "source": [
    "上面三张图是rbg三个通道的展示结果，下面我们合并成一个通道再看一下"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f781327a",
   "metadata": {},
   "outputs": [],
   "source": [
    "gag = rgb2gray(ag)\n",
    "plt.imshow()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1436d4e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpg = rgb2gary(pg)\n",
    "plt.imshow()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3817933c",
   "metadata": {},
   "outputs": [],
   "source": [
    "gng = rgb2gray(ng)\n",
    "plt.imshow()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a15fcf9",
   "metadata": {},
   "source": [
    "####  CAM（Class Activation Map）\n",
    "这个方法严格来说不是基于梯度的，但是后面我们会将反向传播与CAM整合，所以简单的对CAM做个说明。\n",
    "\n",
    "CAM（class activation map）是指输入中的什么区域能够指示CNN进行正确的识别。\n",
    "\n",
    "通常特征图上每个位置的值在存在其感知野里面某种模式时被激活，最后的class activation map是这些模式的线性组合，我们可以通过上采样，将class activation map 还原到与原图一样的大小，通过叠加，我们就可以知道哪些区域是与最后分类结果息息相关的部分。\n",
    "\n",
    "\n",
    "#### Grad-CAM\n",
    "顾名思义 Grad-CAM的加权系数是通过反向传播得到的，而CAM的特征加权系数是分类器的权值。\n",
    "\n",
    "Grad-CAM 与 CAM相比，它的优点是适用的范围更广，Grad-CAM对各类结构，各种任务都可以使用。这两种方法也可以应用于进行弱监督下的目标检测，后续也有相关工作基于它们进行改进来做弱监督目标检测。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f85fc382",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from gradcam import GradCam\n",
    "from guided_gradcam import guided_grad_cam\n",
    "from guided_backprop import GuidedBackprop\n",
    "nlayers = len(net.features._modules.item()) - 1 \n",
    "print(nlayers)\n",
    "cam_list = []\n",
    "\n",
    "for layer in range(nlayers):\n",
    "    # GradCam\n",
    "    grad_cam = GradCam(net, target_layer = layer)\n",
    "    cam = grad_cam.generate_cam(inputs, label)\n",
    "\n",
    "    # GuidedBackprop\n",
    "    GBP = GuidedBackprop(net)\n",
    "    guided_grads = GBP.generate_gradients(inputs, label)\n",
    "\n",
    "    # Guided Grad Cam\n",
    "    cam_gb = guided_grad_cam(cam, guided_grads)\n",
    "    cam_list.append(rgb2gray(np.moveaxis(cam_gb, 0, -1)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee896413",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(cam_list[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "667e2cd6",
   "metadata": {},
   "source": [
    "需要注意的是，在使用 Visualizing and Understanding Convolutional Networks的时候，对网络模型是有要求的，要求网络将模型包含名为features的组合层，这部分是代码中写死的，所以在pytorch的内置模型中，vgg、alexnet、densenet、squeezenet是可以直接使用的，inception(googlenet)和resnet没有名为features的组合层，如果要使用的话是需要对代码进行修改的。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3cb399f",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}


RAG（Retrieval-Augmented Generation）的核心是将模型内部学到的“参数化”知识与外部的知识库的非参数化知识相结合，其运作逻辑就是在LLM生成文本之前，先通过检索机制从外部知识库中动态获取相关信息，并将这些参考资料融入生成过程。

## 1. 什么是RAG？
### 1.1 技术原理
主要通过两个阶段来完成这一过程。
1. 检索阶段：寻找非参数化知识。
- 知识向量化：嵌入模型（Embedding Model）充当了”连接器“的角色。它将外部的知识库编码为向量索引（Index），存入向量数据库。
- 语义召回：当用户发起查询时，检索模块利用同样的嵌入模型将问题向量化，并通过相似度搜索（Similarity Search），从海量数据中精准锁定与问题最相关的文档片段。

2. 生成阶段：融合两种知识
   - 上下文整合：生成模块接受检索阶段送来的相关文档片段以及用户的原始问题。
   - 指令引导生成：该模块回遵循预设的Prompt指令，将上下文与问题有效整合，并引导出LLM进行可控的，有理有据的文本生成。

```
[ 用户提问 ] 
     ↓
[ 检索模块 ] 🔍 ——→ ( 从数据库抓取相关文档 )
     ↓
[ 增强提示词 ] 📝 ——→ ( 问题 + 相关文档 )
     ↓
[ 大模型 LLM ] 🤖
     ↓
[ 最终回答 ]
```


### 1.2 技术演进分类

![[Pasted image 20260212094713.png]]


|      | 初级RAG             | 高级RAG                                   | 模块化RAG                                                       |
| ---- | ----------------- | --------------------------------------- | ------------------------------------------------------------ |
| 流程   | 离线：索引<br>在线：检索-生成 | 离线：索引<br>在线：... - 检索前 - ... - 检索后 - ... | 积木式可编排流程                                                     |
| 特点   | 基础线性流程            | 增加检索前后的优化步骤                             | 模块化，可组合，可动态调整                                                |
| 关键技术 | 基础向量检索            | 查询重写（query rewrite）<br>结果重排（rerank）     | 动态路由（Routing）<br>查询转换（query transformation）<br>多路融合 （Fusion） |
| 局限性  | 效果不稳定，难以优化        | 流程相对固定，优化点有限                            | 系统复杂性高                                                       |

## 2. 为什么使用RAG？

### 2.1 技术选型：RAG vs. 微调
在选择具体的技术路径时，一个重要的考量就是成本与效益的平衡。通常，我们应该优先选择对模型改动最小，成本最低的方案，所以技术选型路径往往是遵循的提示词工程（Prompt Engineering）-> 检索增强生成 -> 微调（Fine-tuning）。

![[Pasted image 20260212100145.png]]

横轴代表LLM优化，即对模型本身进行多大程度的修改。提示词工程和RAG完全不改变模型权重，而微调直接修改模型参数。

纵轴代表上下文优化，是对输入给模型的信息进行多大程度的增强。

我们的技术选择路径一般如下：
- 先尝试提示词工程：通过精心设计提示词来引导模型，适用于任务简单，模型已有相关知识的场景。
- 再选择RAG：如果模型缺乏特定或实时知识而无法回答，则使用RAG，通过外挂知识库为其提供上下文信息。
- 最后考虑微调：当目标是改变模型“如何做”（行为/风格/格式）而不是“知道什么”时，微调时最终且最合适的选择。例如，让模型学会严格遵守某种独特的输出格式，模仿特定人物的对话风格，或者将及其复杂的指令蒸馏进权重中。


## 3. 如何上手？

### 3.1 基础工具选择
构建RAG系统通常涉及几个关键环节的选型。在开发模式上，我们可以利用LangChain或LlamaIndex等成熟框架快速集成，也可以选择不依赖框架的原生开发。

而在向量数据库方面，既有Milvus，Pinecone等适合大规模数据的方案，也有FAISS，Chroma等轻量级或本地化的选择，后期为了效果，还可以引入RAGAS或TruLens等自动化评估工具。

### 3.2 四步构建最小可行系统（MVP）
1. 数据准备与清洗：这是系统的地基，我们需要将PDF，Word等多元异构数据标准化，并采用合理的分块策略（如按语义段落切分而非固定字符数），避免信息在切割中支离破碎。
2. 索引构建：将切分好的文本通过嵌入模型转化为向量，并存入数据库。可以在此阶段关联元数据，这对后续的精确引用很有帮助。
3. 检索策略优化：不要依赖单一的向量搜索。可以采用混合检索等方式提升召回率，并引入重排序模型对检索结果进行二次精选。
4. 生成与提示词工程：最后，设计一套清晰的Prompt模板。引导LLM基于检索到的上下文回答用户问题，并明确要求模型不知道说不知道，防止幻觉。

### 3.3 进阶挑战
1. 评估维度与挑战
业界通常会从 几个维度进行量化评估，首先是检索相关性（找到的内容是否包含答案），其次是生成质量，又可以细分为语义准确性和词汇匹配度。

2. 优化方向与架构演进
- 性能方面：可以通过索引分层（对高频数据启用缓存）和多模态扩展（支持图像/表格检索）来提升效率和能力边界。
- 架构层面：简单的线性流程正在被复杂的设计模式所取代。例如，系统可以通过分支模式并行处理多路检索，或通过循环模式进行自我修正。
{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b20486f5",
   "metadata": {},
   "source": [
    "### 1. 聚类的概念\n",
    "对于有标签的数据，我们进行有监督学习，常见的分类任务就是监督学习；而对于无标签的数据，我们希望发现无标签的数据中的潜在信息，这就是无监督学习。聚类，就是无监督学习的一种，它的概念是：将相似的对象归到同一个簇中，使得同一个簇内的数据对象的相似性尽可能大，同时不在同一个簇中的数据对象的差异性也尽可能地大。即聚类后同一类的数据尽可能聚集到一起，不同数据尽量分离。\n",
    "\n",
    "### 2. 聚类算法的分类\n",
    "聚类算法有很多种分法,常见的分类方法有：\n",
    "1. 基于划分的聚类：聚类目标是使得类内的点足够近，类间的点足够远，常见的如k-means及其衍生算法\n",
    "2. 基于密度的聚类：当临近区域的密度超过某个阈值，则继续聚类，如DBSCAN，OPTICS\n",
    "3. 层次聚类：包括合并的层次聚类，分裂的聚类层次，实际上可以看作是二叉树的生成和分裂过程。\n",
    "4. 基于图的聚类：通过建图来进行聚类，这是聚类算法中的大头，很多较新的聚类算法都有图聚类的思想。\n",
    "\n",
    "### 3.性能度量\n",
    "在机器学习中，我们都要对任务进行评价以便于进行下一步的优化，聚类的性能度量主要有以下两种：\n",
    "- 外部指标：是指把算法得到的划分结果跟某个外部的“参考模型”（如专家给出的划分结果）比较。其实质就是分析分错了和分对了的比例来衡量聚类效果、\n",
    "- 内部指标：是指直接考察聚类结果，不利用任何参考模型的指标。例如轮廓系数：衡量了每个簇中的紧凑度，以及簇间的距离。当每个簇中距离越紧凑，每个簇间距离越远则认为聚类效果优秀。\n",
    "\n",
    "### 4.距离计算\n",
    "在机器学习和数据挖掘中，我们经常需要知道个体间差异的大小，进而评价个体的相似性和类别。\n",
    "- 欧式距离（2-norm距离）：欧氏距离是最易于理解的一种距离计算方法，源自欧氏空间中两点间的距离公式。\n",
    "$$d(x, y) = \\sqrt{\\sum_{k=1}^{n} (x_k - y_k)^2}$$\n",
    "- 曼哈顿距离（Manhattan distance, 1-norm距离）:曼哈顿距离也称为街区距离，是数学中比较常用的距离度量之一，用以表示标准坐标系下两点之间的轴距和。计算公式如下\n",
    "$$d(x, y) = \\sum_{k=1}^{n} \\lvert x_k - y_k \\rvert$$\n",
    "- 切比雪夫距离：切比雪夫距离和曼哈顿距离类似，但其采用的是两点之间的最大轴距。\n",
    "\n",
    "$$d(x, y) = \\lim_{r \\to \\infty} \\left( \\sum_{k=1}^{n} (|x_k - y_k|^r) \\right)^{1/r}\n",
    "        = \\max_k (|x_k - y_k|)$$\n",
    "\n",
    "- 闵可夫斯基距离（Minkowski Distance）$$d(x, y) = \\left( \\sum_{k=1}^{n} (|x_k - y_k|^r) \\right)^{1/r}$$ 其中$r$是一个可变参数，根据参数$r$的不同取值，闵可夫斯基距离可以表示一类距离：\n",
    "\n",
    "  - 当$r = 1$时，为 **曼哈顿距离**\n",
    "  - 当$r = 2$时，为 **欧式距离**\n",
    "  - 当$r \\to \\infty$时，为 **切比雪夫距离**\n",
    " \n",
    " 闵可夫斯基距离包括欧式距离、曼哈顿距离、切比雪夫距离都假设数据各维属性的量纲和分布（期望、方差）相同，因此适用于度量独立同分布的数据对象。\n",
    "- 余弦相似性：余弦相似度公式定义如下：$$cos(x, y)= \\frac{xy}{\\lvert x \\rvert \\lvert y \\rvert} =\\frac {\\sum_{k=1}^{n} x_k y_k}{\\sqrt{\\sum_{k=1}^{n} x_k^2}\\sqrt{\\sum_{k=1}^{n} y_k^2}}$$ 余弦相似度实际上是向量x和y夹角的余弦度量，可用来衡量两个向量方向的差异。如果余弦相似度为1，则x和y之间夹角为0°，两向量除模外可认为是相同的；如果预先相似度为0，则x和y之间夹角为90°，则认为两向量完全不同。在计算余弦距离时，将向量均规范化成具有长度11，因此不用考虑两个数据对象的量值。 余弦相似度常用来度量文本之间的相似性。文档可以用向量表示，向量的每个属性代表一个特定的词或术语在文档中出现的频率，尽管文档具有大量的属性，但每个文档向量都是稀疏的，具有相对较少的非零属性值。\n",
    "\n",
    "- 马氏距离：$mahalanobis(x,y) = (x-y) \\Sigma^{-1}(x-y)^T$ 式中，$\\Sigma^{-1}$是数据协方差矩阵的逆。 前面的距离度量方法大都假设样本独立同分布、数据属性之间不相关。马氏距离考虑了数据属性之间的相关性，排除了属性间相关性的干扰，而且与量纲无关。若协方差矩阵是对角阵，则马氏距离变成了标准欧式距离；若协方差矩阵是单位矩阵，各个样本向量之间独立同分布，则变成欧式距离。\n",
    "\n",
    "  \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc2d85db",
   "metadata": {},
   "source": [
    "### 5. K-means\n",
    "对于K-Means算法，首先要注意的是k值的选择，一般来说，我们会根据对数据的先验经验选择一个合适的k值，如果没有什么先验知识，则可以通 过交叉验证选择一个合适的k值。在确定了k的个数后，我们需要选择k个初始化的质心。由于我们是启发式方法，k个初始化的质心的位置选择对最后的聚类结果和运行时间都有很大的影响，因此需要选择合适的k个质心，最好这些质心不能太近。\n",
    "\n",
    "总结下传统的K-Means算法的流程为：\n",
    "- 输入样本$D = {x_1, x_2, ... ,x_m}$，聚类的簇数$k$，最大迭代次数$N$\n",
    "- 输出的是簇划分$C = {C_1, C_2, ..., C_k}$\n",
    "  \n",
    "  1. 从数据集$D$中随机选择$k$个样本作为初始的$k$个质心向量：${\\mu_1, \\mu_2, ..., \\mu_k}$\n",
    "  2. 对于$n = 1, 2, ..., N$\n",
    "   \n",
    "     1.将簇划分$C$初始化为$C_t = \\varnothing,\\quad t = 1, 2, \\ldots, k$\n",
    "\n",
    "     2.对于$i = 1, 2, \\ldots, m$，计算样本$x_i$和各个质心向量$\\mu_j(j=1,2,\\ldots,k)$的距离$d_{ij} = \\| x_i - \\mu_j \\|_2^2$，将$x_i$标记到使$d_{ij}$最小的类别$\\lambda_i$，即更新簇：$C_{\\lambda_i} = C_{\\lambda_i} \\cup \\{ x_i \\}$\n",
    "\n",
    "     3.对于$j = 1, 2, \\ldots, k$，对簇$C_j$中所有样本点重新计算质心：$\\mu_j = \\frac{1}{|C_j|} \\sum_{x \\in C_j} x$\n",
    "\n",
    "     1. 若所有$k$个质心向量均无变化，则结束；否则返回步骤 3。\n",
    "     2. \n",
    "  3. 输出簇划分$C={C_1, C_2,...,C_k}$\n",
    "   \n",
    "### 6. K-means的优缺点\n",
    "- 优点\n",
    "    - 原理比较简单，实现也是很容易，收敛速度快。\n",
    "    - 聚类效果较优。\n",
    "    - 算法的可解释度比较强。\n",
    "    - 主要需要调参的参数仅仅是簇数k。\n",
    "\n",
    "- 缺点\n",
    "    - K值的选取不好把握\n",
    "    - 最终结果和初始点的选择有关，容易陷入局部最优。\n",
    "    - 对噪音和异常点比较的敏感。\n",
    "    - 数据必须符合“数据之间的相似度可以使用欧式距离衡量”，这个是什么意思呢，看下图，这种数据的分布，样本点的距离不能简单地用欧式距离来衡量，否则分类效果会非常差。这里的距离衡量应该是“测地距离”，也就是样本沿着曲面到达另一个样本点的距离。如果在这种数据空间想要使用kmeans，必须先进行空间的转化。\n",
    "  \n",
    "k-means有一些改进算法，多是针对k-means会受异常点的影响这一点来改进的，这里就不详细赘述，只是简单提一下，起一个抛砖引玉的作用。\n",
    "\n",
    "| 缺点           | 改进       | 描述                                                         |\n",
    "|----------------|------------|--------------------------------------------------------------|\n",
    "| k 值的确定     | ISODATA    | 当属于某个簇的样本数过少时把这个簇去除；当属于某个簇的样本数过多、分散程度较大时把这个簇分为两个子簇                    | \n",
    "| 对奇异点敏感   | k-median   | 以中位数代替平均值作为簇中心                                 |\n",
    "| 只能找到球状簇 | GMM        | 以高斯分布考虑簇内数据点的分布                               |\n",
    "| 分群结果不稳定 | K-means++  | 初始聚类中心之间的相互距离要尽可能的远                       |\n",
    "\n",
    "### 7. 常见面试题\n",
    "#### 7.1 k值的选取\n",
    "K-means算法要求事先知道数据集能分为几群，主要有两种方法定义k。\n",
    "- 手肘法：通过绘制k和损失函数的关系图，选拐点处的k值。\n",
    "- 经验选取人工据经验先定几个k，多次随机初始化中心选经验上最适合的。\n",
    "通常都是以经验选取，因为实际操作中拐点不明显，且手肘法效率不高。\n",
    "\n",
    "#### 7.2 为什么在计算K-means之前要将数据点在各维度上归一化\n",
    "因为数据点各维度的量级不同。 举个例子，最近正好做完基于RFM模型的会员分群，每个会员分别有R（最近一次购买距今的时长）、F（来店消费的频率）和M（购买金额）。如果这是一家奢侈品商店，你会发现M的量级（可能几万元）远大于F（可能平均10次以下），如果不归一化就算K-means，相当于F这个特征完全无效。如果我希望能把常客与其他顾客区别开来，不归一化就做不到。\n",
    "\n",
    "#### 7.3 K-means不适用哪些数据\n",
    "1. 数据特征极强相关的数据集，因为会很难收敛（损失函数是非凸函数），一般要用kernal K-means，将数据点映射到更高维度再分群。\n",
    "2. 数据集可分出来的簇密度不一，或有很多离群值（outliers），这时候考虑使用密度聚类。\n",
    "\n",
    "#### 7.4 K-means中空聚类的处理\n",
    "如果所有的点在指派步骤都未分配到某个簇，就会得到空簇。如果这种情况发生，则需要某种策略来选择一个替补质心，否则的话，平方误差将会偏大。一种方法是选择一个距离当前任何质心最远的点。这将消除当前对总平方误差影响最大的点。另一种方法是从具有最大SEE的簇中选择一个替补的质心。这将分裂簇并降低聚类的总SEE。如果有多个空簇，则该过程重复多次。另外编程实现时，要注意空簇可能导致的程序bug。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1c7f749",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "from sklearn import datasets\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c4f85989",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# 聚类前\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m X = \u001b[43mnp\u001b[49m.random.rand(\u001b[32m100\u001b[39m, \u001b[32m2\u001b[39m)\n\u001b[32m      3\u001b[39m plt.scatter(X[:,\u001b[32m0\u001b[39m], X[:,\u001b[32m1\u001b[39m], marker = \u001b[33m'\u001b[39m\u001b[33mo\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "# 聚类前\n",
    "X = np.random.rand(100, 2)\n",
    "plt.scatter(X[:,0], X[:,1], marker = 'o')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e753938c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 初始化质心，从原有的数据中选取k个作为质心\n",
    "def InitCentroids(X, k):\n",
    "    index = np.random.randint(0, len(X)-1, k)\n",
    "    return X[index]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3422ccc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#聚类后\n",
    "kmeans = KMeans(n_clusters=2).fit(X)\n",
    "label_pred = kmeans.labels_\n",
    "plt.scatter(X[:,0], X[:,1], c=label_pred)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdd158c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba767bb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"] = \"TRUE\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f298ff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = nn.Sequential(nn.Linear(4, 8), nn.ReLU(), nn.Linear(8, 1))\n",
    "X = torch.rand(size=(2, 4))\n",
    "print(net(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c1ddb17",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('1.访问第二个全连接层的参数')\n",
    "print(net[2].state_dict())  # 当通过Sequential类定义模型时，我们可以通过索引来访问模型的任意层\n",
    "print(net[2].bias)  # 第二个神经网络层提取偏置\n",
    "print(net[2].bias.data)  # 第二个神经网络层提取偏置的实际值\n",
    "print(net[2].weight.grad is None)  # 由于我们还没有调用这个网络的反向传播，所以参数的梯度处于初始状态。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b32874a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('2.一次性访问所有参数')\n",
    "print(*[(name, param.shape) for name, param in net[0].named_parameters()])  # 输入层的参数\n",
    "print(*[(name, param.shape) for name, param in net.named_parameters()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc254920",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.嵌套块的参数\n",
    "def block1():\n",
    "    return nn.Sequential(nn.Linear(4, 8), nn.ReLU(),\n",
    "                         nn.Linear(8, 4), nn.ReLU())\n",
    "\n",
    "def block2():\n",
    "    net = nn.Sequential()\n",
    "    for i in range(4):\n",
    "        net.add_module(f'block{i}', block1())\n",
    "    return net\n",
    "\n",
    "rgnet = nn.Sequential(block2(), nn.Linear(4, 1))\n",
    "print(rgnet)\n",
    "print(rgnet(X))\n",
    "print(rgnet[0][1][0].bias.data)  # 访问第一个主要的块，其中第二个子块的第一层的偏置项\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d41653b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.1内置的初始化器\n",
    "def init_normal(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        nn.init.normal_(m.weight, mean=0, std=0.01)\n",
    "        nn.init.zeros_(m.bias)\n",
    "\n",
    "net.apply(init_normal)\n",
    "print(net[0].weight.data[0], net[0].bias.data[0])\n",
    "\n",
    "# 4.2所有参数初始化为给定的常数\n",
    "def init_constant(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        nn.init.constant_(m.weight, 1)\n",
    "        nn.init.zeros_(m.bias)\n",
    "\n",
    "net.apply(init_constant)\n",
    "print(net[0].weight.data[0], net[0].bias.data[0])\n",
    "\n",
    "# 4.3使用Xavier初始化方法初始化第一层，然后第二层初始化为常量值42\n",
    "def xavier(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        nn.init.constant_(m.weight, 42)\n",
    "\n",
    "net[0].apply(xavier)\n",
    "net[2].apply(init_42)\n",
    "print(net[0].weight)\n",
    "print(net[2].weight.data)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "190f73de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.参数自定义初始化\n",
    "def my_init(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        print(\"Init\", *[(name, param.shape) for name, param in m.named_parameters()][0])\n",
    "        nn.init.uniform_(m.weight, -10, 10)\n",
    "        m.weight.data *= m.weight.data.abs() >= 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2ba046f",
   "metadata": {},
   "outputs": [],
   "source": [
    "net.apply(my_init)\n",
    "print(net[0].weight[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "221e3997",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6.多个层间共享参数\n",
    "# 我们需要给共享层一个名称，以便可以引用它的参数。\n",
    "shared = nn.Linear(8, 8)\n",
    "net = nn.Sequential(nn.Linear(4, 8), nn.ReLU(), shared, nn.ReLU(), shared,\n",
    "                    nn.ReLU(), nn.Linear(8, 1))\n",
    "\n",
    "net(X)\n",
    "\n",
    "# 检查参数是否相同\n",
    "print(net[2].weight.data[0] == net[4].weight.data[0])\n",
    "net[2].weight.data[0, 0] = 100\n",
    "# 我们需要给共享层一个名称，以便可以引用它的参数。\n",
    "print(net[2].weight.data[0] == net[4].weight.data[0])"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e58f4359",
   "metadata": {},
   "source": [
    "## AdaBoost\n",
    "\n",
    "Adaboost是一种boosting算法，与boosting算法相似的有bagging，两者同属于ensemble learning。\n",
    "\n",
    "把准确率不高但在50%以上的分类器成为弱分类器，弱分类器可以是一些基础的ML算法比如单层决策树等。对于统一数据可能有多种弱分类器，`AdaBoost`就是将这些弱分类器结合起来，成为基分类器，从而有效提高整体的准确率。\n",
    "\n",
    "但是要想获得好的结合或者集成，对基分类器的要求是“好而不同”，即有一定的准确率，而且弱分类器之间要有“多样性”，比如一个判断是否为男性的任务，弱分类器1侧重从鼻子、耳朵这些特征判断是否是男人，分类器2侧重脸和眼睛等等，把这些分类器结合起来就有了所有用来判断是否男性的特征，并且adaboost还可以给每个基分类器赋值不同的权重，比如从脸比鼻子更能判断是否为男性，就可以把分类器2的权重调高一些，这也是adaboost需要学习的内容。\n",
    "\n",
    "## 数学推导\n",
    "\n",
    "`AdaBoost`可以表示为基分类器的线性组合： $$ H(\\boldsymbol{x})=\\sum_{i=1}^{N} \\alpha_{i} h_{i}(\\boldsymbol{x}) $$ 其中$h_i(x),i=1,2,...$表示基分类器，$\\alpha_i$是每个基分类器对应的权重，表示如下： $$ \\alpha_{i}=\\frac{1}{2} \\ln \\left(\\frac{1-\\epsilon_{i}}{\\epsilon_{i}}\\right) $$ 其中$\\epsilon_{i}$是每个弱分类器的错误率。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42b5e49e",
   "metadata": {},
   "source": [
    "boosting是一族将弱学习器提升为强学习器的算法。这族算法的工作机制是：先从初试训练集训练出一个基学习器，再根据基学习器的表现对训练样本进行调整，使得先前基学习器做错的训练样本在后续受到更多关注，然后基于调整后的样本分布来训练下一个基分类器，如此重复进行，直至基学习器数目达到实现指定值T，最后将这T个基学习器进行加权组合。\n",
    "\n",
    "boosting族算法最著名的代表室AdaBoost,AdaBoost算法有多种推导方式，比较容易理解的是基于“加性模型”，即基学习器的线性组合:\n",
    "\n",
    "$$\n",
    "H(x) = \\sum_{t=1}^{T} \\alpha_t h_t(x)\n",
    "$$\n",
    "\n",
    "指数损失函数为：\n",
    "\n",
    "$$l_{\\exp}(H \\mid D) = \\mathbb{E}_{X \\sim D} \\left[ e^{-f(x) H(x)} \\right]$$\n",
    "\n",
    "#### 算法步骤\n",
    "\n",
    "**输入：**\n",
    "训练集\n",
    "$$D = (x_1, y_1), (x_2, y_2), \\ldots, (x_m, y_m)$$\n",
    "**基学习算法**\n",
    "$$\\xi$$\n",
    "**训练轮次**\n",
    "$$T$$\n",
    "\n",
    "\n",
    "#### 过程\n",
    "\n",
    "1. **初始化权重分布**\n",
    "\n",
    "\n",
    "$$D_1(x) = \\frac{1}{m}$$\n",
    "\n",
    "\n",
    "2. **for**\n",
    "\n",
    "\n",
    "$$t = 1,2,3,\\ldots,T$$\n",
    "\n",
    "\n",
    "**do:**\n",
    "\n",
    "3. 训练弱分类器\n",
    "\n",
    "\n",
    "$$h_t = \\xi(D, D_t)$$\n",
    "\n",
    "\n",
    "4. 计算分类误差\n",
    "\n",
    "\n",
    "$$\\epsilon_t = \\mathbb{P}_{X \\sim D_t}(h_t(x) \\neq f(x))$$\n",
    "\n",
    "\n",
    "5. 若\n",
    "\n",
    "\n",
    "$$\\epsilon_t > 0.5$$\n",
    "\n",
    "\n",
    "则 **break**\n",
    "\n",
    "6. 计算弱分类器系数\n",
    "\n",
    "\n",
    "$$\\alpha_t = \\frac{1}{2} \\ln\\left(\\frac{1 - \\epsilon_t}{\\epsilon_t}\\right)$$\n",
    "\n",
    "\n",
    "7. 更新分布\n",
    "\n",
    "\n",
    "$$D_{t+1}(x) = \\frac{ D_t(x) \\exp(-\\alpha_t f(x) h_t(x)) }{ Z_t }$$\n",
    "\n",
    "\n",
    "8. **end for**\n",
    "\n",
    "\n",
    "## 输出\n",
    "\n",
    "最终分类器：\n",
    "\n",
    "$$H(x) = \\operatorname{sign}\\left( \\sum_{t=1}^{T} \\alpha_t h_t(x) \\right)$$\n",
    "\n",
    "## 补充说明\n",
    "\n",
    "其中：\n",
    "\n",
    "* ( \\epsilon_t ) 是弱分类器在分布 (D_t) 下的错误率\n",
    "* ( Z_t ) 为归一化因子，用以确保 (D_{t+1}) 是合法概率分布\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26567931",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64aec3d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#构建并训练决策树分类器，这里特征选择标准使用基尼指数，树的最大深度为1\n",
    "base_model = DecisionTreeClassifier(max_depth = 1, criterion='gini', random_state=1).fix(X_train, y_train)\n",
    "y_pred = base_model.predict(X_test)\n",
    "print(f\"决策树的准确率：{accuracy_score(y_test, y_pred):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87259f1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.datasets import load_wine\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "import pandas as pd\n",
    "\n",
    "wine = load_wine() #使用葡萄酒数据集\n",
    "print(f\"所有特征：{wine.feature_names}\")\n",
    "X = pd.DataFrame(wine.data, columns = wine.feature_names)\n",
    "y = pd.Series(wine.target)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state = 1)\n",
    "\n",
    "print(f\"训练数据量：{len(X_train)}, 测试数据量：{len(X_test)}\")\n",
    "\n",
    "# 定义模型，这里最大分类器数量为50，学习率为1.5\n",
    "model = AdaBoostClassifier(base_estimator = base_model, n_estimators = 50, learning_rate = 0.8)\n",
    "# 训练\n",
    "model.fit(X_train, y_train)\n",
    "# 预测\n",
    "y_pred = model.predict(X_test)\n",
    "acc = metrics.accuracy_score(y_test, y_pred)\n",
    "print(f\"准确率：{acc:.2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cabc9ce",
   "metadata": {},
   "source": [
    "### 使用GridSearchCV自动调参"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3151055e",
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameter_space = {'n_estimators': list(range(2, 102, 2)),\n",
    "                        'learning_rate':[0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1]}\n",
    "\n",
    "# 使用准确率为标准，将得到的准确率最高的参数输出，cv=5表示交叉验证参数，这里使用五折交叉验证，n_jobs=-1表示并行数和cpu一致\n",
    "gs = GridSearchCV(AdaBoostClassifier(\n",
    "    algorithm = 'SAMME.R',\n",
    "    random_state=1),\n",
    "    param_grid=hyperparameter_space,\n",
    "    scoring = \"accuracy\", n_jobs=-1, cv = 5)\n",
    "\n",
    "gs.fit(X_train, y_train)\n",
    "print(\"最优超参数：\", gs.best_params_)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

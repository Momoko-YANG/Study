{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6c74e8cc",
   "metadata": {},
   "source": [
    "### 1.降维的基本介绍\n",
    "机器学习领域中所谓的降维就是指采用某种映射方法，将原高维空间中的数据点映射到低维度的空间中。降维的本质是学习一个映射函数 f : x->y，其中x是原始数据点的表达，目前最多使用向量表达形式。 y是数据点映射后的低维向量表达，通常y的维度小于x的维度（当然提高维度也是可以的）。f可能是显式的或隐式的、线性的或非线性的。\n",
    "\n",
    "关于维度灾难的一个理解角度：\n",
    "假设存在下面这样一个球，D维，其半径为r=1，里面圆环的长度为$\\varepsilon$\n",
    "则，我们计算可以得知：\n",
    "$$V_{外} = k * 1^D = k$$\n",
    "\n",
    "$$V_{环} = V_{外} - V_{内} = k - k*(1-\\epsilon)^D$$\n",
    "\n",
    "$$\\frac{V_{环}}{V_{外}} = \\frac{k - k*(1-\\epsilon)^D}{k} = 1 - (1-\\epsilon)^D$$\n",
    "\n",
    "K 为一个常数。由于 $0 < \\varepsilon < 1$，因此$\\lim_{D \\rightarrow \\infty} (1 - \\varepsilon)^D = 0$,于是$\\lim_{D \\rightarrow \\infty} \\frac{V_{\\text{环}}}{V_{\\text{外}}} = 1$。这就是所谓的维度灾难。在高维数据中，大部分样本都分布在超立方体的边缘，因此数据集更加稀疏。\n",
    "\n",
    "那从另外一个角度来看：为什么dimension reduction可能是有用的，如上图所示，假设你的data分布是这样的(在3D里面像螺旋的样子)，但是用3D空间来描述这些data其实是很浪费的，其实你从资源就可以说：你把这个类似地毯卷起来的东西把它摊开就变成这样(右边的图)。所以你只需要在2D的空间就可以描述这个3D的information，你根本不需要把这个问题放到这个3D来解，这是把问题复杂化，其实你可以在2D就可以做这个task"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c45fcb7b",
   "metadata": {},
   "source": [
    "### 1.1 常见降维算法比较\n",
    "降维算法分为：\n",
    "1. 直接降维，特征选择\n",
    "2. 线性降维，PCA，MDS等\n",
    "3. 分线性，流形包括lsomap，LLE等\n",
    "\n",
    "| 算法名称       | 线性/非线性 | 有监督/无监督 | (超)参数       | 是否去中心化 | 目标                                                         | 假设                                                                 | 涉及矩阵          | 解的形式                                                             |\n",
    "|----------------|-------------|----------------|----------------|--------------|--------------------------------------------------------------|----------------------------------------------------------------------|-------------------|-----------------------------------------------------------------------|\n",
    "| PCA            | 线性        | 无监督         | W, d           | 是           | 降维后的低维样本之间每一维方差尽可能大                      | 低维空间相互正交                                                     | C, W             | 取 C 前 d 个最大特征值对应特征向量排成线性变换 W 的列               |\n",
    "| MDS            | 非线性      | 无监督         | d              | 是           | 降维同时保证数据之间的相对关系不变                           | 已知高维空间的样本之间的距离矩阵                                    | E, A             | 取 E 前 d 个最大特征值对应特征向量排成低维矩阵 A                    |\n",
    "| LDA            | 线性        | 有监督         | W, d           | 否           | 降维后一类样本之间协方差尽可能小，不同类中心距离尽可能大   | 数据能够被分成 k+1 类                                                 | $S_w, S_b, W$  | 求  $S_w^{-1} S_b$ 特征分解的前 d 个最大特征值对应特征向量排成 W |\n",
    "| Isomap         | 非线性      | 无监督         | d, k           | 是           | 降维同时保持高维数据的流形不变                               | 高维空间的局部区域上，某两点距离以图上的 geodesic 距离估算           | E, A             | 与 MDS 一致                                                           |\n",
    "| LLE            | 非线性      | 无监督         | k              | 是           | 降维同时保持高维数据的流形不变                               | 某点的高维邻域在低维空间内也能线性组合近邻点                         | F, M             | 取 M 前 d 个非 0 最小特征值对应特征向量构成                          |\n",
    "| t-SNE          | 非线性      | 无监督         | K = 2/3        | —            | 降维到二维或者三维可视化                                    | 高维空间中，一个点的概率质量主要从另一个点中心的高斯邻域中获得；低维空间中，用 t 分布的欧式距离反映从自由度为 1 的分布 | P, Q             | 梯度下降方式求解并不断更新低维空间的 Q                              |\n",
    "| Autoencoder    | 非线性      | 无监督         | $ W_l, I_l, D_l$  | —        | 降维后尽可能重构输入数据                                    | 网络能够学习到数据中的某些特性或结构                               | $W_l$         | 网络最后一层的输出                                                    |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ded0ada",
   "metadata": {},
   "source": [
    "### 1.2 sklearn中的降维算法\n",
    "| 类别                 | 模块/类名                                   | 说明                                           |\n",
    "|----------------------|----------------------------------------------|------------------------------------------------|\n",
    "| **主成分分析**       | decomposition.PCA                            | 主成分分析（PCA）                               |\n",
    "|                      | decomposition.IncrementalPCA                 | 增量主成分分析（IPCA）                          |\n",
    "|                      | decomposition.KernelPCA                      | 核主成分分析（KPCA）                            |\n",
    "|                      | decomposition.MiniBatchSparsePCA            | 小批量稀疏主成分分析                             |\n",
    "|                      | decomposition.SparsePCA                     | 稀疏主成分分析（SparsePCA）                      |\n",
    "|                      | decomposition.TruncatedSVD                  | 截断的 SVD（aka LSA）                            |\n",
    "|                      |                                              |                                                |\n",
    "| **因子分析**         | decomposition.FactorAnalysis                 | 因子分析（FA）                                   |\n",
    "|                      |                                              |                                                |\n",
    "| **独立成分分析**     | decomposition.FastICA                        | 独立成分分析的快速算法                           |\n",
    "|                      |                                              |                                                |\n",
    "| **字典学习**         | decomposition.DictionaryLearning             | 字典学习                                        |\n",
    "|                      | decomposition.MiniBatchDictionaryLearning    | 小批量字典学习                                   |\n",
    "|                      | decomposition.dict_learning                  | 字典学习用于矩阵分解                             |\n",
    "|                      | decomposition.dict_learning_online           | 在线字典学习用于矩阵分解                         |\n",
    "|                      |                                              |                                                |\n",
    "| **高级矩阵分解**     | decomposition.LatentDirichletAllocation      | 具备在线变分贝叶斯算法的隐含狄利克雷分布模型      |\n",
    "|                      | decomposition.NMF                            | 非负矩阵分解（NMF）                              |\n",
    "|                      |                                              |                                                |\n",
    "| **其他矩阵分解**     | decomposition.SparseCoder                    | 稀疏编码                                        |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "255a7c09",
   "metadata": {},
   "source": [
    "### 2.主成分分析PCA\n",
    "#### 2.1 损失函数\n",
    "我们假设数据集为 $$ X_{N\\times p}=(x_{1},x_{2},\\cdots,x_{N})^{T},x_{i}=(x_{i1},x_{i2},\\cdots,x_{ip})^{T} $$ 这个记号表示有$N$个样本，每个样本都是$p$维向量。其中每个观测都是由$p(x|\\theta)$生成的。\n",
    "\n",
    "为了方便，我们首先将协方差矩阵（数据集）写成中心化的形式： $$ \\begin{aligned} S &=\\frac{1}{N} \\sum_{i=1}^{N}\\left(x_{i}-\\bar{x}\\right)\\left(x_{i}-\\bar{x}\\right)^{T} \\ &=\\frac{1}{N}\\left(x_{1}-\\bar{x}, x_{2}-\\bar{x}, \\cdots, x_{N}-\\bar{x}\\right)\\left(x_{1}-\\bar{x}, x_{2}-\\bar{x}, \\cdots, x_{N}-\\bar{x}\\right)^{T} \\ &=\\frac{1}{N}\\left(X^{T}-\\frac{1}{N} X^{T} \\mathbb{I}{N 1} \\mathbb{I}{N 1}^{T}\\right)\\left(X^{T}-\\frac{1}{N} X^{T} \\mathbb{I}{N 1} \\mathbb{I}{N 1}^{T}\\right)^{T} \\ &=\\frac{1}{N} X^{T}\\left(E_{N}-\\frac{1}{N} \\mathbb{I}{N 1} \\mathbb{I}{1 N}\\right)\\left(E_{N}-\\frac{1}{N} \\mathbb{I}{N 1} \\mathbb{I}{1 N}\\right)^{T} X \\ &=\\frac{1}{N} X^{T} H_{N} H_{N}^{T} X \\ &=\\frac{1}{N} X^{T} H_{N} H_{N} X=\\frac{1}{N} X^{T} H X \\end{aligned} $$ 这个式子利用了中心矩阵$H$的对称性，这也是一个投影矩阵。\n",
    "\n",
    "主成分分析中，我们的基本想法是将所有数据投影到一个字空间中，从而达到降维的目标，为了寻找这个子空间，我们基本想法是：\n",
    "1. 所有数据在子空间中更为分散\n",
    "2. 损失的信息最小，即：在补空间的分量少\n",
    "\n",
    "原来的数据很有可能各个维度之间是相关的，于是我们希望找到一组$p$个新的线性无关的单位基$u_i$，降维就是取其中的$q$个基。于是对于一个样本$x_i$，经过这个坐标变换后： $$ \\hat{x_i}=\\sum\\limits_{i=1}^p(u_i^Tx_i)u_i=\\sum\\limits_{i=1}^q(u_i^Tx_i)u_i+\\sum\\limits_{i=q+1}^p(u_i^Tx_i)u_i $$ 对于数据集来说，我们首先将其中心化然后再去上面的式子的第一项，并使用其系数的平方平均作为损失函数并最大化： $$ \\begin{aligned} J &=\\frac{1}{N} \\sum_{i=1}^{N} \\sum_{j=1}^{q}\\left(\\left(x_{i}-\\bar{x}\\right)^{T} u_{j}\\right)^{2} \\ &=\\sum_{j=1}^{q} u_{j}^{T} S u_{j}, \\text { s.t. } u_{j}^{T} u_{j}=1 \\end{aligned} $$ 由于每个基都是线性无关的，于是每一个$u_j$的求解可以分别进行，使用拉格朗日乘子法： $$ \\mathop{argmax}{u_j}L(u_j,\\lambda)=\\mathop{argmax}{u_j}u_j^TSu_j+\\lambda(1-u_j^Tu_j) $$ 于是： $$ Su_j=\\lambda u_j $$ 可见，我们需要的基就是协方差矩阵的本征矢。损失函数最大取在本征值前$q$个最大值。\n",
    "\n",
    "下面看其损失的信息最少这个条件，同样适用系数的平方平均作为损失函数，并最小化： $$ \\begin{aligned} J &=\\frac{1}{N} \\sum_{i=1}^{N} \\sum_{j=q+1}^{p}\\left(\\left(x_{i}-\\bar{x}\\right)^{T} u_{j}\\right)^{2} \\ &=\\sum_{j=q+1}^{p} u_{j}^{T} S u_{j}, \\text { s.t. } u_{j}^{T} u_{j}=1 \\end{aligned} $$ 同样的： $$ \\mathop{argmin}{u_j}L(u_j,\\lambda)=\\mathop{argmin}{u_j}u_j^TSu_j+\\lambda(1-u_j^Tu_j) $$ 损失函数最小取在本征值剩下的个最小的几个值。数据集的协方差矩阵可以写成$S = U\\Lambda U^T$，直接对这个表达式当然可以得到本征矢。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6debb28d",
   "metadata": {},
   "source": [
    "### 2.2 SVD 与 PCoA\n",
    "下面使用实际训练时常常使用的 SVD 直接求得这个q个本征矢。\n",
    "\n",
    "对中心化后的数据集进行奇异值分解： $$ HX=U\\Sigma V^T,U^TU=E_N,V^TV=E_p,\\Sigma:N\\times p $$\n",
    "\n",
    "于是： $$ S=\\frac{1}{N}X^THX=\\frac{1}{N}X^TH^THX=\\frac{1}{N}V\\Sigma^T\\Sigma V^T $$ 因此，我们直接对中心化后的数据集进行 SVD，就可以得到特征值和特征向量$V$，在新坐标系中的坐标就是： $$ HX\\cdot V $$ 由上面的推导，我们也可以得到另一种方法 PCoA 主坐标分析，定义并进行特征值分解： $$ T=HXX^TH=U\\Sigma\\Sigma^TU^T $$ 由于： $$ TU\\Sigma=U\\Sigma(\\Sigma^T\\Sigma) $$ 于是可以直接得到坐标。这两种方法都可以得到主成分，但是由于方差矩阵是$p×p$的，而$T$是$N×N$的，所以对样本量较少的时候可以采用PCoA的方法。\n",
    "\n",
    "\n",
    "总结来说就是\n",
    "\n",
    "输入: 样本集 $D = \\left\\{ \\boldsymbol{x}_1, \\boldsymbol{x}_2, \\ldots, \\boldsymbol{x}_m \\right\\}$; 低维空间维数$d^{'}$\n",
    "\n",
    "过程：\n",
    "1. 对所有样本进行中心化：  \n",
    "   $\\boldsymbol{x}_i \\leftarrow \\boldsymbol{x}_i - \\frac{1}{m}\\sum_{j=1}^{m} \\boldsymbol{x}_j$；\n",
    "2. 计算样本的协方差矩阵 $XX^{T}$；\n",
    "3. 对协方差矩阵 $XX^{T}$ 进行特征分解；\n",
    "4. 取最大的 $d'$ 个特征值所对应的特征向量$\\boldsymbol{w}_1, \\boldsymbol{w}_2, \\ldots, \\boldsymbol{w}_{d'}$。\n",
    "\n",
    "输出：投影矩阵  \n",
    "$\\mathbf{W} = \\left( \\boldsymbol{w}_1, \\boldsymbol{w}_2, \\ldots, \\boldsymbol{w}_{d'} \\right)$。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "579b6b45",
   "metadata": {},
   "source": [
    "### 2.3 p-PCA\n",
    "\n",
    "下面从概率的角度对 PCA 进行分析，概率方法也叫 p-PCA。我们使用线性模型，类似之前 LDA，我们选定一个方向，对原数据 $x \\in \\mathbb{R}^{p}$，降维后的数据为 $z \\in \\mathbb{R}^{q}, q < p$。降维通过一个矩阵变换（投影）进行：\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "&z \\sim \\mathcal{N}(\\mathbb{O}_{q \\times 1}, \\mathbb{I}_{q \\times q}) \\\\\n",
    "&x = W z + \\mu + \\varepsilon \\\\\n",
    "&\\varepsilon \\sim \\mathcal{N}(0, \\sigma^{2} \\mathbb{I}_{p \\times p})\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "对于这个模型，我们可以使用期望-最大 (EM) 的算法进行学习，在进行推断的时候需要求得 $p(z|x)$，推断的求解过程和线性高斯模型类似。\n",
    "\n",
    "$$\n",
    "\\begin{gathered}\n",
    "p(z \\mid x) = \\frac{p(x \\mid z) p(z)}{p(x)} \\\\\n",
    "\\mathbb{E}[x] = \\mathbb{E}[W z + \\mu + \\varepsilon] = \\mu \\\\\n",
    "\\operatorname{Var}[x] = W W^{T} + \\sigma^{2} \\mathbb{I}_{p} \\\\\n",
    "\\Longrightarrow p(z \\mid x) = \\mathcal{N}\\left(W^{T}\\left(W W^{T} + \\sigma^{2} \\mathbb{I}\\right)^{-1}(x - \\mu), \\mathbb{I} - W^{T}\\left(W W^{T} + \\sigma^{2} \\mathbb{I}\\right)^{-1} W\\right)\n",
    "\\end{gathered}\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab4e1d37",
   "metadata": {},
   "source": [
    "## 3. sklearn代码实践"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e318e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#首先我们生成随机数据并可视化\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "%matplotlib inline\n",
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "# X为样本特征，Y为样本簇类别， 共1000个样本，每个样本3个特征，共4个簇\n",
    "X, y = make_blobs(n_samples=10000, n_features=3, centers=[[3, 3, 3],[0, 0, 0],[1, 1, 1],[2, 2, 2]], cluster_std=[0.2, 0.1, 0.2, 0.2], random_state=9)\n",
    "fig = plt.figure()\n",
    "ax = Axes3D(fig, rect=[0, 0, 1, 1], elev=30, azim=20)\n",
    "plt.scatter(X[:,0], X[:, 1], X[:, 2], marker='o')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d556a688",
   "metadata": {},
   "source": [
    "我们先不降维，只对数据进行投影，看看投影后的三个维度的方差分布，代码如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4afddc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=3)\n",
    "pca.fit(X)\n",
    "print(pca.explained_variance_ratio_)\n",
    "print(pca.explained_variance_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41237b6a",
   "metadata": {},
   "source": [
    "可以看出投影后三个特征维度的方差比例大约为98.3%：0.8%：0.8%。投影后第一个特征占了绝大多数的主成分比例。\n",
    "\n",
    "现在我们来进行降维，从三维降到2维，代码如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42a7c6f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=2)\n",
    "pca.fit(X)\n",
    "print(pca.explained_variance_ratio_)\n",
    "print(pca.explained_variance_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a77fbfd",
   "metadata": {},
   "source": [
    "这个结果其实可以预料，因为上面三个投影后的特征维度的方差分别为：[ 3.78483785 0.03272285 0.03201892]，投影到二维后选择的肯定是前两个特征，而抛弃第三个特征。\n",
    "\n",
    "为了有个直观的认识，我们看看此时转化后的数据分布，代码如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66fcf2c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_new = pca.transform(X)\n",
    "plt.scatter(X_new[:,0], X_new[:,1], marker='o')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf3b0a38",
   "metadata": {},
   "source": [
    "可见降维后的数据依然可以很清楚的看到我们之前三维图中的4个簇。\n",
    "\n",
    "现在我们看看不直接指定降维的维度，而指定降维后的主成分方差和比例。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a77243f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=0.95)\n",
    "pca.fit(X)\n",
    "print(pca.explained_variance_ratio_)\n",
    "print(pca.explained_variance_)\n",
    "print(pca.n_components_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b841bebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=0.99)\n",
    "pca.fit(X)\n",
    "print(pca.explained_variance_ratio_)\n",
    "print(pca.explained_variance_)\n",
    "print(pca.n_components_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cd4c448",
   "metadata": {},
   "source": [
    "这个结果也很好理解，因为我们第一个主成分占了98.3%的方差比例，第二个主成分占了0.8%的方差比例，两者一起可以满足我们的阈值。\n",
    "\n",
    "最后我们看看让MLE算法自己选择降维维度的效果，代码如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59449216",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components='mle')\n",
    "pca.fit(X)\n",
    "print(pca.explained_variance_ratio_)\n",
    "print(pca.explained_variance_)\n",
    "print(pca.n_components_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4f62dac",
   "metadata": {},
   "source": [
    "可见由于我们的数据的第一个投影特征的方差占比高达98.3%，MLE算法只保留了我们的第一个特征。"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

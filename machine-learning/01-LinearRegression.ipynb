{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "64f6486d",
   "metadata": {},
   "source": [
    "## 线性回归\n",
    "\n",
    "线性回归是比较简单的机器学习算法，属于线性模型的一部分。\n",
    "\n",
    "### 线性模型定义\n",
    "\n",
    "> 线性模型：给定由d个属性描述的示例 \\(\\boldsymbol{x}=\\left(x_{1} ; x_{2} ; \\ldots ; x_{d}\\right)\\)，其中 \\(x_i\\) 是 \\(x\\) 在第i个属性上的取值，线性模型(linear model) 试图学得一个通过属性的线性组合来进行预测的函数，即\n",
    "> \n",
    "> \\[\n",
    "> f(\\boldsymbol{x})=w_{1} x_{1}+w_{2} x_{2}+\\ldots+w_{d} x_{d}+b\n",
    "> \\]\n",
    "\n",
    "简单地说就是找到一条直线使得这些点都在这条直线上或者直线附近。\n",
    "\n",
    "线性回归就是线性模型中最经典的模型之一。\n",
    "\n",
    "### 问题定义\n",
    "\n",
    "> 给定数据集 \\(D=\\left\\{\\left(\\boldsymbol{x}_{1}, y_{1}\\right),\\left(\\boldsymbol{x}_{2}, y_{2}\\right), \\ldots,\\left(\\boldsymbol{x}_{m}, y_{m}\\right)\\right\\}\\)，其中 \\(\\boldsymbol{x}_{i}=\\left(x_{i 1}; x_{i 2} ; \\ldots ; x_{i d}\\right), y_{i} \\in \\mathbb{R}\\)\n",
    "\n",
    "\"线性回归\" (linear regression) 试图学得一个线性模型以尽可能准确地预测实值输出标记。\n",
    "\n",
    "按照线性模型的思想，设x是特征，y是真实值，我们需要找到一条尽可能让 \\(f(x)=y\\) 的曲线 \\(f(x)=wx+b\\)，也就是找到对应的w和b。关键在于衡量 \\(f(x)\\) 与 \\(y\\) 的差别。\n",
    "\n",
    "### 最小二乘法\n",
    "\n",
    "均方误差是回归任务中最常用的性能度量，因此我们可试图让均方误差最小化，即 \n",
    "\n",
    "\\[\n",
    "\\begin{aligned}\n",
    "\\left(w^{*}, b^{*}\\right) &= \\underset{(w, b)}{\\arg \\min } \\sum_{i=1}^{m}\\left(f\\left(x_{i}\\right)-y_{i}\\right)^{2} \\\\\n",
    "&= \\underset{(w, b)}{\\arg \\min } \\sum_{i=1}^{m}\\left(y_{i}-w x_{i}-b\\right)^{2}\n",
    "\\end{aligned}\n",
    "\\]\n",
    "\n",
    "均方误差有非常好的几何意义，它对应了常用的欧几里得距离。基于均方误差最小化来进行模型求解的方法称为“最小二乘法”。在线性回归中，最小二乘法就是试图找到一条直线，使所有样本到直线上的欧式距离之和最小。\n",
    "\n",
    "求解$w$和$b$使$E_{(w, b)}=\\sum_{i=1}^{m}\\left(y_{i}-w x_{i}-b\\right)^{2}$最小化的过程，称为线性回归模型的最小二乘“参数估计”。我们可将$E_{(w,b)}$分别对$w$和$b$求导，得到： \n",
    "\n",
    "\\[\n",
    "\\begin{aligned}\n",
    "\\frac{\\partial E_{(w, b)}}{\\partial w} &= 2\\left(w \\sum_{i=1}^{m} x_{i}^{2}-\\sum_{i=1}^{m}\\left(y_{i}-b\\right) x_{i}\\right) \\\\\n",
    "\\frac{\\partial E_{(w, b)}}{\\partial b} &= 2\\left(m b-\\sum_{i=1}^{m}\\left(y_{i}-w x_{i}\\right)\\right)\n",
    "\\end{aligned}\n",
    "\\]\n",
    "\n",
    "然后令上两式为0可得到 \\(w\\) 和 \\(b\\) 最优解的闭式解：\n",
    "\n",
    "\\[\n",
    "w=\\frac{\\sum_{i=1}^{m} y_{i}\\left(x_{i}-\\bar{x}\\right)}{\\sum_{i=1}^{m} x_{i}^{2}-\\frac{1}{m}\\left(\\sum_{i=1}^{m} x_{i}\\right)^{2}}\n",
    "\\]\n",
    "\n",
    "其中\n",
    "\n",
    "\\[\n",
    "\\bar{x}=\\frac{1}{m} \\sum_{i=1}^{m} x_{i}\n",
    "\\]\n",
    "\n",
    "为 \\(x\\) 的均值。\n",
    "\n",
    "更一般的情形是如本节开头的数据集D，样本由d个属性描述，此时我们试图学得\n",
    "\n",
    "\\[\n",
    "f\\left(\\boldsymbol{x}_{i}\\right)=\\boldsymbol{w}^{\\mathrm{T}} \\boldsymbol{x}_{i}+b, \\text{ 使得 } f\\left(\\boldsymbol{x}_{i}\\right) \\simeq y_{i}\n",
    "\\]\n",
    "\n",
    "这就是多元线性回归。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec63e85b",
   "metadata": {},
   "source": [
    "## 1.1 数据生成\n",
    "生成数据的思路是设定一个二维的函数（维度高了没办法在平面上画出来），根据这个函数生成一些离散的数据点，对每个数据点我们可以适当的加一点波动，也就是噪声，最后看看我们算法的拟合或者说回归效果。"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

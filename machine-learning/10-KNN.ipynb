{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "23e42901",
   "metadata": {},
   "source": [
    "### 1. KNN算法简介\n",
    "KNN的全称是K Nearest Neighbors，意思是K个最近的邻居，可以用于分类和回归，是一种监督学习算法。它的思路是这样，如果一个样本在特征空间中的K个最相似(即特征空间中最邻近)的样本中的大多数属于某一个类别，则该样本也属于这个类别。也就是说，该方法在定类决策上只依据最邻近的一个或者几个样本的类别来决定待分样本所属的类别。\n",
    "\n",
    "### 2. KNN算法介绍\n",
    "#### 2.1 KNN算法三要素\n",
    "##### 2.1.1 关于距离的衡量方法：具体介绍参见K-means介绍\n",
    "KNN算法中要求数据的所有特征都可以做量化，若在数据特征中存在非数值类型,必须采用手段将其量化为数值。\n",
    "\n",
    "在`sklearn`中，KNN分类器提供了四种距离\n",
    "- 欧式距离(euclidean) - 默认\n",
    "- 曼哈顿距离(manhatten)\n",
    "- 切比雪夫距离(chebyshev)\n",
    "- 闵可夫斯基距离(minkowski)\n",
    "\n",
    "##### 2.1.2 K值的选择问题\n",
    "在KNN分类中，K值的选择往往没有一个固定的经验，可以通过不停调整(例如交叉验证)到一个合适的K值。\n",
    "\n",
    "1. K为1。如果K值被设定为1，那么训练集的正确率将达到100%(将训练集同时作为预测集)，因为每个点只会找到它本身，但同时在预测集中的正确率不会太高（极度过拟合）。\n",
    "2. K为较小的值。较小的邻域往往会带来更低的训练误差，但会导致过拟合的问题降低预测集的准确率。\n",
    "3. K为较大的值。较大的邻域会增大训练误差，但能够有效减少过拟合的问题。（注意，这并不意味着预测集的准确率一定会增加）\n",
    "4. K为训练集样本数量。当K极端到邻域覆盖整个样本时，就相当于不再分类而直接选择在训练集中出现最多的类。\n",
    "\n",
    "在实际应用中，K 值一般选择一个较小的数值，通常采用交叉验证的方法来选择最优的 K 值。随着训练实例数目趋向于无穷和 K=1 时，误差率不会超过贝叶斯误差率的2倍，如果K也趋向于无穷，则误差率趋向于贝叶斯误差率。（贝叶斯误差可以理解为最小误差）\n",
    "\n",
    "三种交叉验证方法：\n",
    "- Hold-Out： 随机从最初的样本中选出部分，形成交叉验证数据，而剩余的就当做训练数据。 一般来说，少于原本样本三分之一的数据被选做验证数据。常识来说，Holdout 验证并非一种交叉验证，因为数据并没有交叉使用。\n",
    "- K-foldcross-validation：K折交叉验证，初始采样分割成K个子样本，一个单独的子样本被保留作为验证模型的数据，其他K-1个样本用来训练。交叉验证重复K次，每个子样本验证一次，平均K次的结果或者使用其它结合方式，最终得到一个单一估测。这个方法的优势在于，同时重复运用随机产生的子样本进行训练和验证，每次的结果验证一次，10折交叉验证是最常用的。\n",
    "- Leave-One-Out Cross Validation：正如名称所建议， 留一验证(LOOCV)意指只使用原本样本中的一项来当做验证资料， 而剩余的则留下来当做训练资料。 这个步骤一直持续到每个样本都被当做一次验证资料。 事实上，这等同于 K-fold 交叉验证是一样的，其中K为原本样本个数。\n",
    "\n",
    "##### 2.1.3 分类决策的准则\n",
    "明确K个邻居中所有数据类别的个数，将测试数据划分给个数最多的那一类。即由输入实例的 K 个最临近的训练实例中的多数类决定输入实例的类别。 最常用的两种决策规则：\n",
    "\n",
    "- 多数表决法：多数表决法和我们日常生活中的投票表决是一样的，少数服从多数，是最常用的一种方法。\n",
    "- 加权表决法：有些情况下会使用到加权表决法，比如投票的时候裁判投票的权重更大，而一般人的权重较小。所以在数据之间有权重的情况下，一般采用加权表决法。\n",
    "\n",
    "#### 2.2  KNN算法的步骤\n",
    "\n",
    "输入：训练数据集 \n",
    "$T = \\left\\{ (x_1, y_1), (x_2, y_2), \\ldots, (x_N, y_N) \\right\\}$，其中 $x_i$ 为实例的特征向量，$y_i \\in \\left\\{ c_1, c_2, \\ldots, c_k \\right\\}$ 为实例类别。\n",
    "\n",
    "\n",
    "输出：实例$x$所属的类别$y$。\n",
    "\n",
    "步骤：\n",
    "1. 选择参数K\n",
    "2. 计算未知实例与所有已知实例的距离（可选择多种计算距离的方式）\n",
    "3. 选择最近K个已知实例\n",
    "4. 根据少数服从多数的投票法则(majority-voting)，让未知实例归类为K个最近邻样本中最多数的类别。加权表决法同理。\n",
    "\n",
    "#### 2.3 KNN算法的优缺点\n",
    "\n",
    "优点：\n",
    "- KNN是一种较为成熟的算法，同时思路也比较简单，能够同时兼容回归与分类（KNN的回归将在日后的回归算法中提到）。\n",
    "- KNN时间复杂度为O(n)。因为是懒惰学习，在训练阶段速度比较快。\n",
    "- 可以用于线性分类。\n",
    "- 未对数据进行任何假设，因此对异常点不敏感。\n",
    "- 通过近邻而不是通过类域判别，对类域交叉重叠“较多的样本具有较好的预测效果”。\n",
    "\n",
    "缺点：\n",
    "- 在特征较多的情况下，会有很大的计算量。\n",
    "- 需要储存所有的训练数据，对内存要求高。\n",
    "- 因为是懒惰学习（在测试样本阶段学习），预测阶段速度比较慢。\n",
    "- 在样本不平衡是，容易造成误判。\n",
    "- 对数据规模敏感。在大的训练集中有较高正确率，当规模小的时候正确率低。\n",
    "\n",
    "### 3.常见面试题\n",
    "#### 3.1 为了解决KNN算法计算量过大的问题，可以使用分组的方式进行计算，简述一下该方式的原理。\n",
    "先将样本按距离分解成组，获得质心，然后计算未知样本到各质心的距离，选出距离最近的一组或几组，再在这些组内引用KNN。\n",
    "本质上就是事先对已知样本点进行剪辑，事先去除对分类作用不大的样本，该方法比较适用于样本容量比较大时的情况。\n",
    "\n",
    "#### 3.2 K-Means与KNN有什么区别\n",
    "- KNN\n",
    "     - KNN是分类算法\n",
    "     - 监督学习\n",
    "     - 喂给它的数据集是带label的数据，已经是完全正确的数据\n",
    "     - 没有明显的前期训练过程，属于memory-based learning\n",
    "     - K的含义：来了一个样本x，要给它分类，即求出它的y，就从数据集中，在x附近找离它最近的K个数据点，这K个数据点，类别c占的个数最多，就把x的label设为c\n",
    "- K-Means\n",
    "     - K-Means是聚类算法\n",
    "     - 非监督学习\n",
    "     - 喂给它的数据集是无label的数据，是杂乱无章的，经过聚类后才变得有点顺序，先无序，后有序\n",
    "     - 有明显的前期训练过程\n",
    "     - K的含义：K是人工固定好的数字，假设数据集合可以分为K个簇，由于是依靠人工定好，需要一点先验知识\n",
    "- 相似点：都包含这样的过程，给定一个点，在数据集中找离它最近的点。即二者都用到了NN(Nears Neighbor)算法，一般用KD树来实现NN。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57c69443",
   "metadata": {},
   "source": [
    "### 4. 算法实践（sklearn）\n",
    "#### 4.1 KNeighborsClassifier 类\n",
    "`sklearn`库的`neighbors`模块实现了`KNN`相关算法，其中：\n",
    "- `KNeighborsClassifier`类用于分类问题\n",
    "- `KNeighborsRegressor`类用于回归问题\n",
    "\n",
    "这两个类的构造方法基本一致，这里我们主要介绍 KNeighborsClassifier 类，原型如下：\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "1cbe4783",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "KNeighborsClassifier(\n",
    "    n_neighbors=5,\n",
    "    weights='uniform',\n",
    "    algorithm='auto',\n",
    "    leaf_size=30,\n",
    "    p=2,\n",
    "    metric='minkowski',\n",
    "    metric_params=None,\n",
    "    n_jobs=None,\n",
    "    **kwargs\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "187f2c59",
   "metadata": {},
   "source": [
    "来看下几个重要参数的含义：\n",
    "- n_neighbors：即 KNN 中的 K 值，一般使用默认值 5。\n",
    "- weights：用于确定邻居的权重，有三种方式：\n",
    "  - weights=uniform，表示所有邻居的权重相同。\n",
    "  - weights=distance，表示权重是距离的倒数，即与距离成反比。\n",
    "  - 自定义函数，可以自定义不同距离所对应的权重，一般不需要自己定义函数。\n",
    "- algorithm：用于设置计算邻居的算法，它有四种方式：\n",
    "   - algorithm=auto，根据数据的情况自动选择适合的算法。\n",
    "   - algorithm=kd_tree，使用KD树算法。\n",
    "      - KD 树是一种多维空间的数据结构，方便对数据进行检索。\n",
    "      - KD 树适用于维度较少的情况，一般维数不超过 20，如果维数大于 20 之后，效率会下降。\n",
    "   - algorithm=ball_tree，使用球树算法。\n",
    "      - 与KD 树一样都是多维空间的数据结构。\n",
    "      - 球树更适用于维度较大的情况。\n",
    "   - algorithm=brute，称为暴力搜索。\n",
    "      - 它和 KD 树相比，采用的是线性扫描，而不是通过构造树结构进行快速检索。\n",
    "      - 缺点是，当训练集较大的时候，效率很低。\n",
    "   - leaf_size：表示构造 KD 树或球树时的叶子节点数，默认是 30。 调整 leaf_size 会影响树的构造和搜索速度。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8587b5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载数据集\n",
    "from sklearn.datasets import load_digits\n",
    "digits = load_digits()\n",
    "data = digits.data #特征集\n",
    "target = digits.target # 目标集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a78df80f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier() #采用默认参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43fc6234",
   "metadata": {},
   "outputs": [],
   "source": [
    "#拟合模型：\n",
    "knn.fit(train_x, train_y)\n",
    "\n",
    "#预测数据：\n",
    "predict_y = knn.predict(test_x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1da1f15e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 计算模型准确度\n",
    "from sklearn.metrics import accuracy_score\n",
    "score = accuracy_score(test_y, predict_y)\n",
    "score"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
